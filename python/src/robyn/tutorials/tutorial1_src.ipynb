{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robyn: Marketing Mix Modeling Application\n",
    "\n",
    "This notebook demonstrates the usage of Robyn, a Marketing Mix Modeling (MMM) application. \n",
    "We'll go through the main steps of performing robyn_inputs and robyn_engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Import Required Libraries. Define Paths.\n",
    "\n",
    "First, be sure to setup your virtual environment. Be sure to switch over to your new environment in this notebook. \n",
    "\n",
    "-```cd {root_folder}```\n",
    "\n",
    "-```python3 -m yourvenv```\n",
    "\n",
    "-```source yourvenv/bin/activate```\n",
    "\n",
    "-```cd Robyn/python```\n",
    "\n",
    "-```pip install -r requirements.txt```\n",
    "\n",
    "\n",
    "Then import the necessary libraries. Make sure to define your paths below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/yijuilee/robynpy_release_reviews/Robyn/python/src\")\n",
    "\n",
    "import pandas as pd\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.data.entities.enums import AdstockType\n",
    "from robyn.data.entities.holidays_data import HolidaysData\n",
    "from robyn.data.entities.hyperparameters import Hyperparameters, ChannelHyperparameters\n",
    "from robyn.modeling.entities.modelrun_trials_config import TrialsConfig\n",
    "from robyn.modeling.model_executor import ModelExecutor\n",
    "from robyn.modeling.entities.enums import NevergradAlgorithm, Models\n",
    "from robyn.modeling.feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Mock R data\n",
    "\n",
    "We need to set the base path for the data directory.\n",
    "Create a .env file in the same directory as your notebook and put in define the path to the data dir.\n",
    "for example: ROBYN_BASE_PATH=.../Robyn/R/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the simulated data and holidays data\n",
    "dt_simulated_weekly = pd.read_csv(\"resources/dt_simulated_weekly.csv\")\n",
    "\n",
    "dt_prophet_holidays = pd.read_csv(\"resources/dt_prophet_holidays.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MMM Data\n",
    "\n",
    "We will now set up the MMM data specification which includes defining the dependent variable, independent variables, and the time window for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mmm_data(dt_simulated_weekly) -> MMMData:\n",
    "\n",
    "    mmm_data_spec = MMMData.MMMDataSpec(\n",
    "        dep_var=\"revenue\",\n",
    "        dep_var_type=\"revenue\",\n",
    "        date_var=\"DATE\",\n",
    "        context_vars=[\"competitor_sales_B\", \"events\"],\n",
    "        paid_media_spends=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"],\n",
    "        paid_media_vars=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_I\", \"search_clicks_P\"],\n",
    "        organic_vars=[\"newsletter\"],\n",
    "        window_start=\"2016-01-01\",\n",
    "        window_end=\"2018-12-31\",\n",
    "    )\n",
    "\n",
    "    return MMMData(data=dt_simulated_weekly, mmmdata_spec=mmm_data_spec)\n",
    "\n",
    "\n",
    "mmm_data = setup_mmm_data(dt_simulated_weekly)\n",
    "mmm_data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing\n",
    "\n",
    "We will perform feature engineering to prepare the data for modeling. This includes transformations like adstock and other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    {\n",
    "        \"facebook_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"print_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"tv_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.3, 0.8],\n",
    "        ),\n",
    "        \"search_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"ooh_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"newsletter\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "    },\n",
    "    adstock=AdstockType.GEOMETRIC,\n",
    "    lambda_=[0, 1],\n",
    "    train_size=[0.5, 0.8],\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HolidaysData object\n",
    "holidays_data = HolidaysData(\n",
    "    dt_holidays=dt_prophet_holidays,\n",
    "    prophet_vars=[\"trend\", \"season\", \"holiday\"],\n",
    "    prophet_country=\"DE\",\n",
    "    prophet_signs=[\"default\", \"default\", \"default\"],\n",
    ")\n",
    "# Setup FeaturizedMMMData\n",
    "feature_engineering = FeatureEngineering(mmm_data, hyperparameters, holidays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_mmm_data = feature_engineering.perform_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.visualization.feature_visualization import FeaturePlotter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a FeaturePlotter instance\n",
    "feature_plotter = FeaturePlotter(mmm_data, hyperparameters, featurized_mmm_data)\n",
    "# Extract the list of results\n",
    "results_list = featurized_mmm_data.modNLS[\"results\"]\n",
    "# Plot spend-exposure relationship for each channel in the results\n",
    "for result in results_list:\n",
    "    channel = result[\"channel\"]\n",
    "    try:\n",
    "        fig = feature_plotter.plot_spend_exposure(channel)\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {channel}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ModelExecutor\n",
    "model_executor = ModelExecutor(\n",
    "    mmmdata=mmm_data,\n",
    "    holidays_data=holidays_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    calibration_input=None,  # Add calibration input if available\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    ")\n",
    "\n",
    "# Setup TrialsConfig\n",
    "trials_config = TrialsConfig(\n",
    "    iterations=2000, trials=5\n",
    ")  # Set to the number of cores you want to use\n",
    "\n",
    "print(\n",
    "    f\">>> Starting {trials_config.trials} trials with {trials_config.iterations} iterations each using {NevergradAlgorithm.TWO_POINTS_DE.value} nevergrad algorithm on...\"\n",
    ")\n",
    "\n",
    "# Run the model\n",
    "\n",
    "output_models = model_executor.model_run(\n",
    "    trials_config=trials_config,\n",
    "    ts_validation=True,  # changed from True to False -> deacitvate\n",
    "    add_penalty_factor=False,\n",
    "    rssd_zero_penalty=True,\n",
    "    cores=8,\n",
    "    nevergrad_algo=NevergradAlgorithm.TWO_POINTS_DE,\n",
    "    intercept=True,\n",
    "    intercept_sign=\"non_negative\",\n",
    "    model_name=Models.RIDGE,\n",
    ")\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Display the MOO Distribution Plot\n",
    "if \"moo_distrb_plot\" in output_models.convergence:\n",
    "    moo_distrb_plot = output_models.convergence[\"moo_distrb_plot\"]\n",
    "    display(Image(data=base64.b64decode(moo_distrb_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Display the MOO Cloud Plot\n",
    "if \"moo_cloud_plot\" in output_models.convergence:\n",
    "    moo_cloud_plot = output_models.convergence[\"moo_cloud_plot\"]\n",
    "    display(Image(data=base64.b64decode(moo_cloud_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Display time series validation and convergence plots\n",
    "# if \"ts_validation_plot\" in output_models.convergence:\n",
    "#     ts_validation_plot = output_models.convergence[\"ts_validation_plot\"]\n",
    "#     display(Image(data=base64.b64decode(ts_validation_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.modeling.pareto.pareto_optimizer import ParetoOptimizer\n",
    "\n",
    "# 3. Create ParetoOptimizer instance\n",
    "pareto_optimizer = ParetoOptimizer(\n",
    "    mmm_data, output_models, hyperparameters, featurized_mmm_data, holidays_data\n",
    ")\n",
    "\n",
    "# 4. Run optimize function\n",
    "pareto_result = pareto_optimizer.optimize(pareto_fronts=\"auto\", min_candidates=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.modeling.clustering.clustering_config import ClusteringConfig, ClusterBy\n",
    "from robyn.modeling.clustering.cluster_builder import ClusterBuilder\n",
    "from robyn.data.entities.enums import DependentVarType\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "cluster_configs = ClusteringConfig(\n",
    "    dep_var_type=DependentVarType(mmm_data.mmmdata_spec.dep_var_type),\n",
    "    cluster_by=ClusterBy.HYPERPARAMETERS,\n",
    "    max_clusters=10,\n",
    "    min_clusters=3,\n",
    "    weights=[1.0, 1.0, 1.0],\n",
    ")\n",
    "\n",
    "cluster_builder = ClusterBuilder(pareto_result=pareto_result)\n",
    "\n",
    "\n",
    "cluster_results = cluster_builder.cluster_models(cluster_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your pareto result, look at\n",
    "print(\"Pareto coefficients and decomposition:\")\n",
    "print(pareto_result.x_decomp_agg)\n",
    "\n",
    "print(\"Cluster assignments:\")\n",
    "print(cluster_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reestablish Pareto Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.modeling.pareto.pareto_utils import ParetoUtils\n",
    "\n",
    "utils = ParetoUtils()\n",
    "pareto_result = utils.process_pareto_clustered_results(\n",
    "    pareto_result,\n",
    "    clustered_result=cluster_results,\n",
    "    ran_cluster=True,\n",
    "    ran_calibration=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Different Optimization Scenarios\n",
    "\n",
    "### Scenario 1: Default Max Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.allocator.entities.allocation_params import AllocatorParams\n",
    "from robyn.allocator.entities.allocation_result import (\n",
    "    AllocationResult,\n",
    "    OptimOutData,\n",
    "    MainPoints,\n",
    ")\n",
    "from robyn.allocator.entities.optimization_result import OptimizationResult\n",
    "from robyn.allocator.entities.constraints import Constraints\n",
    "from robyn.allocator.optimizer import BudgetAllocator\n",
    "from robyn.allocator.constants import (\n",
    "    SCENARIO_MAX_RESPONSE,\n",
    "    ALGO_SLSQP_AUGLAG,\n",
    "    CONSTRAINT_MODE_EQ,\n",
    "    DEFAULT_CONSTRAINT_MULTIPLIER,\n",
    "    DATE_RANGE_ALL,\n",
    ")\n",
    "\n",
    "select_model = pareto_result.pareto_solutions[1]\n",
    "\n",
    "# Create allocator parameters matching R Example 1\n",
    "allocator_params = AllocatorParams(\n",
    "    scenario=SCENARIO_MAX_RESPONSE,\n",
    "    total_budget=None,  # When None, uses total spend in date_range\n",
    "    target_value=None,\n",
    "    date_range=\"all\",\n",
    "    channel_constr_low=[0.7],  # Single value for all channels\n",
    "    channel_constr_up=[1.2, 1.5, 1.5, 1.5, 1.5],  # Different values per channel\n",
    "    channel_constr_multiplier=3.0,\n",
    "    optim_algo=\"SLSQP_AUGLAG\",\n",
    "    maxeval=100000,\n",
    "    constr_mode=CONSTRAINT_MODE_EQ,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "# Initialize budget allocator\n",
    "max_response_allocator = BudgetAllocator(\n",
    "    mmm_data=mmm_data,\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    pareto_result=pareto_result,\n",
    "    select_model=select_model,\n",
    "    params=allocator_params,\n",
    ")\n",
    "\n",
    "## Step 3: Run Optimization\n",
    "max_response_result = max_response_allocator.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Step 4: Analyze Results\n",
    "print(\"\\nOptimization Results Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Model ID: {select_model}\")\n",
    "print(f\"Scenario: {max_response_result.scenario}\")\n",
    "print(f\"Use case: {max_response_result.usecase}\")\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Channel\": max_response_result.dt_optimOut.channels,\n",
    "        \"Initial Spend\": max_response_result.dt_optimOut.init_spend_unit,\n",
    "        \"Optimized Spend\": max_response_result.dt_optimOut.optm_spend_unit,\n",
    "        \"Spend Change %\": (\n",
    "            max_response_result.dt_optimOut.optm_spend_unit\n",
    "            / max_response_result.dt_optimOut.init_spend_unit\n",
    "            - 1\n",
    "        )\n",
    "        * 100,\n",
    "        \"Initial Response\": max_response_result.dt_optimOut.init_response_unit,\n",
    "        \"Optimized Response\": max_response_result.dt_optimOut.optm_response_unit,\n",
    "        \"Response Lift %\": (\n",
    "            max_response_result.dt_optimOut.optm_response_unit\n",
    "            / max_response_result.dt_optimOut.init_response_unit\n",
    "            - 1\n",
    "        )\n",
    "        * 100,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results_df.round(2))\n",
    "\n",
    "# Print additional diagnostics\n",
    "print(\"\\nOptimization Parameters:\")\n",
    "print(f\"Total budget: {max_response_allocator.constraints.budget_constraint:,.2f}\")\n",
    "print(\"Bound multiplier:\", max_response_allocator.params.channel_constr_multiplier)\n",
    "print(\"\\nConstraint Violations:\")\n",
    "violations = np.sum(\n",
    "    np.abs(\n",
    "        max_response_result.dt_optimOut.optm_spend_unit\n",
    "        - max_response_allocator.allocator_data_preparer.init_spend_unit\n",
    "    )\n",
    ")\n",
    "print(f\"Total allocation adjustment: {violations:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.visualization.allocator_visualizer import (\n",
    "    AllocatorPlotter,\n",
    ")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Initialize plotter with just the essential data\n",
    "plotter = AllocatorPlotter(\n",
    "    allocation_result=max_response_result,\n",
    "    budget_allocator=max_response_allocator\n",
    ")\n",
    "\n",
    "# Generate all plots\n",
    "plots = plotter.plot_all(display_plots=False, export_location=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
