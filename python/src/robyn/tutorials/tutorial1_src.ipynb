{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robyn: Marketing Mix Modeling in Python\n",
    "\n",
    "Welcome to Robyn, a powerful Marketing Mix Modeling (MMM) tool developed by Meta. This notebook demonstrates how to use Robyn to build and analyze marketing mix models.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "⚠️ **Important**: Before proceeding, please complete the setup steps in the [Python Robyn README](https://github.com/facebookexperimental/Robyn/blob/main/python/README.md). You'll need:\n",
    "1. R installed with the `glmnet` package\n",
    "2. Python virtual environment (recommended)\n",
    "3. Robyn installed via pip or from source\n",
    "\n",
    "This notebook demonstrates the basic workflow of using Robyn for Marketing Mix Modeling. For your own analysis:\n",
    "\n",
    "1. Replace the sample data with your marketing data\n",
    "2. Adjust the model parameters based on your business context\n",
    "3. Validate results against your business knowledge \n",
    "4. Use the optimization insights to guide marketing budget allocation\n",
    "\n",
    "For more information, visit the [Robyn documentation](https://github.com/facebookexperimental/Robyn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Options\n",
    "\n",
    "There are two ways to use this notebook:\n",
    "\n",
    "### Option 1: Using the Published Package\n",
    "If you're using the published `robynpy` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install robynpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Simulated Data with `robynpy` if you are running on Google Colab.\n",
    "\n",
    "After installing the `robynpy` package using pip, you can load the simulated data by following these steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Import Necessary Libraries\n",
    "# import importlib\n",
    "# import robyn\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Step 2: Load the `robyn` Package\n",
    "# pkg = importlib.import_module(\"robyn\")\n",
    "\n",
    "# # Step 3: Define the Path to Tutorials\n",
    "# tutorials_path = pkg.__path__[0] + \"/tutorials\"\n",
    "\n",
    "# # Step 4: Load and Display Simulated Weekly Data\n",
    "# dt_simulated_weekly = pd.read_csv(tutorials_path + \"/resources/dt_simulated_weekly.csv\")\n",
    "# print(\"Simulated Data...\")\n",
    "# print(dt_simulated_weekly.head())\n",
    "\n",
    "# # Step 5: Load and Display Prophet Holidays Data\n",
    "# dt_prophet_holidays = pd.read_csv(tutorials_path + \"/resources/dt_prophet_holidays.csv\")\n",
    "# print(\"Holidays Data...\")\n",
    "# print(dt_prophet_holidays.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using the Development Version\n",
    "If you're working with the source code, add the source directory to Python path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "robyn_path = os.path.abspath(os.path.join(notebook_path, \"../../\"))\n",
    "if robyn_path not in sys.path:\n",
    "    sys.path.append(robyn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Logging for the Development Version\n",
    "\n",
    "This section sets up logging for the Robyn analysis in the development version. The logger will:\n",
    "- Create a `logs` directory in your current working directory\n",
    "- Generate timestamped log files (e.g., `robynpy_20250317_105359.log`)\n",
    "- Record detailed execution information and any warnings/errors\n",
    "\n",
    "The logs can be helpful for:\n",
    "- Debugging issues\n",
    "- Tracking model execution progress\n",
    "- Auditing parameter choices and results\n",
    "- Understanding warning messages\n",
    "\n",
    "You can find the log files in the `./logs` directory after running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 16:25:31,529 - robyn - INFO - Logging is set up to console only.\n",
      "2025-03-18 16:25:31 - __main__ - INFO - Logging initialized - check log directory for detailed logs\n"
     ]
    }
   ],
   "source": [
    "## Configure Logging\n",
    "import os\n",
    "import logging.config\n",
    "from datetime import datetime\n",
    "\n",
    "# Create log directory if it doesn't exist\n",
    "log_dir = os.path.join(os.getcwd(), \"logs\")  # Use relative path in current directory\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Generate a unique log file name using the current date and time\n",
    "log_file_name = f\"robynpy_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "log_file_path = os.path.join(log_dir, log_file_name)\n",
    "\n",
    "# Get the package directory for the logging config\n",
    "import robyn\n",
    "\n",
    "robyn_pkg_dir = os.path.dirname(robyn.__file__)\n",
    "logging_conf_path = os.path.join(robyn_pkg_dir, \"common\", \"config\", \"logging.conf\")\n",
    "\n",
    "# Load the logging configuration with the dynamic log file name\n",
    "logging.config.fileConfig(\n",
    "    logging_conf_path,\n",
    "    defaults={\"logfilename\": log_file_path},\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logging initialized - check log directory for detailed logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Simulated Data from Local Build\n",
    "\n",
    "[Running locally] For this demonstration, we'll use simulated data from our local build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load simulated weekly data\n",
    "dt_simulated_weekly = pd.read_csv(\"resources/dt_simulated_weekly.csv\")\n",
    "\n",
    "# Load holidays data (used for seasonality modeling)\n",
    "dt_prophet_holidays = pd.read_csv(\"resources/dt_prophet_holidays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.data.entities.enums import AdstockType\n",
    "from robyn.data.entities.holidays_data import HolidaysData\n",
    "from robyn.data.entities.hyperparameters import Hyperparameters, ChannelHyperparameters\n",
    "from robyn.modeling.entities.modelrun_trials_config import TrialsConfig\n",
    "from robyn.modeling.model_executor import ModelExecutor\n",
    "from robyn.modeling.entities.enums import NevergradAlgorithm, Models\n",
    "from robyn.modeling.feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll import the core components directly from Robyn's source:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure MMM Data\n",
    "\n",
    "Define the model specification including dependent variables, independent variables, and analysis window. Here's what each parameter means:\n",
    "\n",
    "### Key Components:\n",
    "- `dep_var`: Your target metric (e.g., \"revenue\", \"conversions\")\n",
    "- `dep_var_type`: Type of dependent variable (\"revenue\" for ROI or \"conversion\" for CPA)\n",
    "- `date_var`: Column name containing dates\n",
    "- `window_start/end`: Analysis time period\n",
    "\n",
    "### Variable Types:\n",
    "- `paid_media_spends`: Columns containing media spend data (e.g., TV, Facebook, Search)\n",
    "- `paid_media_vars`: Media exposure metrics (impressions, clicks) in same order as spends\n",
    "- `context_vars`: External factors (e.g., competitor activities, events, seasonality)\n",
    "- `organic_vars`: Marketing activities without direct spend (e.g., email, social posts)\n",
    "\n",
    "Example configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mmm_data(dt_simulated_weekly) -> MMMData:\n",
    "\n",
    "    mmm_data_spec = MMMData.MMMDataSpec(\n",
    "        dep_var=\"revenue\",\n",
    "        dep_var_type=\"revenue\",\n",
    "        date_var=\"DATE\",\n",
    "        context_vars=[\"competitor_sales_B\", \"events\"],\n",
    "        factor_vars=[\"events\"],\n",
    "        paid_media_spends=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"],\n",
    "        paid_media_vars=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_I\", \"search_clicks_P\"],\n",
    "        organic_vars=[\"newsletter\"],\n",
    "        window_start=\"2016-01-01\",\n",
    "        window_end=\"2018-12-31\",\n",
    "    )\n",
    "\n",
    "    return MMMData(data=dt_simulated_weekly, mmmdata_spec=mmm_data_spec)\n",
    "\n",
    "\n",
    "mmm_data = setup_mmm_data(dt_simulated_weekly)\n",
    "mmm_data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hyperparameters\n",
    "\n",
    "Hyperparameters control how media effects are modeled. Each channel requires three key parameters:\n",
    "\n",
    "### Media Channel Parameters:\n",
    "- `alphas`: Controls saturation curve shape [0.5, 3]\n",
    "  - Lower values (0.5-1): More diminishing returns\n",
    "  - Higher values (2-3): More S-shaped response\n",
    "  \n",
    "- `gammas`: Controls saturation curve inflection point [0.3, 1]\n",
    "  - Lower values: Earlier diminishing returns\n",
    "  - Higher values: Later diminishing returns\n",
    "\n",
    "- `thetas`: Controls adstock decay rate [0, 0.8]\n",
    "  - Lower values (0-0.2): Fast decay (e.g., Search, Social)\n",
    "  - Medium values (0.1-0.4): Medium decay (e.g., Print, OOH)\n",
    "  - Higher values (0.3-0.8): Slow decay (e.g., TV)\n",
    "\n",
    "### Global Parameters:\n",
    "- `adstock`: Type of carryover effect modeling\n",
    "  - \"geometric\": Fixed decay rate\n",
    "  - \"weibull_cdf\": Flexible decay with cumulative distribution\n",
    "  - \"weibull_pdf\": Flexible decay with potential peak delay\n",
    "\n",
    "- `lambda_`: Ridge regression regularization [0, 1]\n",
    "- `train_size`: Proportion of data for training [0.5, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    {\n",
    "        \"facebook_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"print_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"tv_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.3, 0.8],\n",
    "        ),\n",
    "        \"search_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"ooh_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"newsletter\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "    },\n",
    "    adstock=AdstockType.GEOMETRIC,\n",
    "    lambda_=[0, 1],\n",
    "    train_size=[0.5, 0.8],\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Holiday Data\n",
    "\n",
    "Holiday data helps capture seasonality and special events in your model. The HolidaysData configuration includes:\n",
    "\n",
    "### Components:\n",
    "- `dt_holidays`: DataFrame containing holiday/events data\n",
    "- `prophet_vars`: Time components to model:\n",
    "  - \"trend\": Long-term trend\n",
    "  - \"season\": Seasonal patterns\n",
    "  - \"holiday\": Holiday/event effects\n",
    "- `prophet_country`: Country code for built-in holidays (e.g., \"DE\" for Germany)\n",
    "- `prophet_signs`: Effect direction for each prophet_var:\n",
    "  - \"default\": Let the model determine direction\n",
    "  - \"positive\": Force positive effect\n",
    "  - \"negative\": Force negative effect\n",
    "\n",
    "Note: You can add custom events (school breaks, promotional periods, etc.) to the holidays data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HolidaysData object\n",
    "holidays_data = HolidaysData(\n",
    "    dt_holidays=dt_prophet_holidays,\n",
    "    prophet_vars=[\"trend\", \"season\", \"holiday\"],\n",
    "    prophet_country=\"DE\",\n",
    "    prophet_signs=[\"default\", \"default\", \"default\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Process\n",
    "\n",
    "The feature engineering step prepares raw marketing data for modeling through four main steps:\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- Standardizes dates and variables\n",
    "- Applies analysis time window\n",
    "- Handles missing values and data types\n",
    "\n",
    "### 2. Time Series Decomposition\n",
    "Uses Prophet to decompose data into:\n",
    "- Trend and seasonality components\n",
    "- Holiday and event effects\n",
    "- Monthly/weekly patterns\n",
    "\n",
    "### 3. Media Variable Processing\n",
    "For channels with different spend vs. exposure metrics:\n",
    "- Fits and compares linear and non-linear models\n",
    "- Selects best model based on fit metrics\n",
    "- Generates spend-exposure relationship plots\n",
    "\n",
    "### 4. Marketing Effect Transformations\n",
    "Applies marketing-specific adjustments:\n",
    "- Adstock transformation for carryover effects\n",
    "- Saturation curves for diminishing returns\n",
    "- Media cost factor normalization\n",
    "\n",
    "The output is a FeaturizedMMMData object containing the transformed features ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup FeaturizedMMMData\n",
    "feature_engineering = FeatureEngineering(mmm_data, hyperparameters, holidays_data)\n",
    "\n",
    "featurized_mmm_data = feature_engineering.perform_feature_engineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Feature Relationships\n",
    "\n",
    "Plot the spend-exposure relationships for each channel to understand how media spend translates to exposure metrics. These plots show:\n",
    "- How media spend relates to exposure (impressions, clicks, etc.)\n",
    "- Model fit quality (R² value)\n",
    "- Whether the relationship follows linear or Michaelis-Menten patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.visualization.feature_visualization import FeaturePlotter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create plotter instance\n",
    "feature_plotter = FeaturePlotter(mmm_data, hyperparameters, featurized_mmm_data)\n",
    "results_list = featurized_mmm_data.modNLS[\"results\"]\n",
    "\n",
    "# Plot each channel\n",
    "for result in results_list:\n",
    "    channel = result[\"channel\"]\n",
    "    try:\n",
    "        # Get the figure dictionary\n",
    "        fig_dict = feature_plotter.plot_spend_exposure(channel)\n",
    "        \n",
    "        if \"spend-exposure\" in fig_dict:\n",
    "            fig = fig_dict[\"spend-exposure\"]\n",
    "            # Need to get the axis from the figure\n",
    "            ax = fig.get_axes()[0]\n",
    "            # Draw the figure to make sure it's rendered\n",
    "            fig.canvas.draw()\n",
    "            # Display using display instead of show\n",
    "            from IPython.display import display\n",
    "            display(fig)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {channel}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {channel}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Training Configuration:\n",
    "- `trials`: Number of parallel optimization trials (e.g., 5)\n",
    "- `iterations`: Optimization iterations per trial (e.g., 2000)\n",
    "- `ts_validation`: Whether to use time-series validation\n",
    "- `cores`: Number of CPU cores to use for parallel processing\n",
    "- `nevergrad_algo`: Optimization algorithm (TWO_POINTS_DE recommended)\n",
    "- `model_name`: Model type (RIDGE regression recommended)\n",
    "\n",
    "Parameters:\n",
    "- `add_penalty_factor`: Additional regularization for stability\n",
    "- `rssd_zero_penalty`: Penalize unrealistic zero contributions\n",
    "- `intercept`: Include intercept term\n",
    "- `intercept_sign`: Control intercept direction\n",
    "\n",
    "Note: More iterations and trials generally lead to better results but increase computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "model_executor = ModelExecutor(\n",
    "    mmmdata=mmm_data,\n",
    "    holidays_data=holidays_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    calibration_input=None,\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    ")\n",
    "\n",
    "trials_config = TrialsConfig(iterations=2000, trials=5)\n",
    "\n",
    "output_models = model_executor.model_run(\n",
    "    trials_config=trials_config,\n",
    "    ts_validation=True,\n",
    "    add_penalty_factor=False,\n",
    "    rssd_zero_penalty=True,\n",
    "    nevergrad_algo=NevergradAlgorithm.TWO_POINTS_DE,\n",
    "    intercept=True,\n",
    "    intercept_sign=\"non_negative\",\n",
    "    model_name=Models.RIDGE,\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Convergence Analysis\n",
    "\n",
    "After training the models, we can examine two key visualization plots that help assess model convergence and optimization quality:\n",
    "\n",
    "### MOO Cloud Plot\n",
    "This plot shows the distribution of model solutions in the objective space:\n",
    "- X-axis: Model fit error (NRMSE) - lower is better\n",
    "- Y-axis: Decomposition error - lower is better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "# Display the MOO Cloud Plot\n",
    "if \"moo_cloud_plot\" in output_models.convergence:\n",
    "    moo_cloud_plot = output_models.convergence[\"moo_cloud_plot\"]\n",
    "    display(Image(data=base64.b64decode(moo_cloud_plot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOO Distribution Plot \n",
    "This plot shows the distribution of objective values across all trials:\n",
    "- Shows the spread of NRMSE and decomposition errors\n",
    "- Helps identify if optimization has converged to stable solutions\n",
    "- Wide distributions may indicate need for more iterations\n",
    "- Narrow distributions suggest consistent model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the MOO Distribution Plot\n",
    "if \"moo_distrb_plot\" in output_models.convergence:\n",
    "    moo_distrb_plot = output_models.convergence[\"moo_distrb_plot\"]\n",
    "    display(Image(data=base64.b64decode(moo_distrb_plot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Clustering\n",
    "\n",
    "### 1. Pareto Optimization\n",
    "Pareto optimization helps select the best models by balancing multiple objectives:\n",
    "- Model accuracy (NRMSE)\n",
    "- Decomposition accuracy\n",
    "- Model robustness\n",
    "\n",
    "Parameters:\n",
    "- `pareto_fronts`: Number of Pareto fronts to consider (\"auto\" recommended)\n",
    "- `min_candidates`: Minimum number of models to retain (e.g., 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.modeling.pareto.pareto_optimizer import ParetoOptimizer\n",
    "\n",
    "# 3. Create ParetoOptimizer instance\n",
    "pareto_optimizer = ParetoOptimizer(\n",
    "    mmm_data, output_models, hyperparameters, featurized_mmm_data, holidays_data\n",
    ")\n",
    "\n",
    "# 4. Run optimize function\n",
    "pareto_result = pareto_optimizer.optimize(pareto_fronts=\"auto\", min_candidates=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Clustering\n",
    "Clustering groups similar models together to identify stable solutions.\n",
    "\n",
    "Configuration parameters:\n",
    "- `dep_var_type`: Type of dependent variable (revenue/conversion)\n",
    "- `cluster_by`: Clustering criterion (HYPERPARAMETERS recommended)\n",
    "- `max_clusters`: Maximum number of clusters to consider\n",
    "- `min_clusters`: Minimum number of clusters\n",
    "- `weights`: Importance weights for clustering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.modeling.clustering.clustering_config import ClusteringConfig, ClusterBy\n",
    "from robyn.modeling.clustering.cluster_builder import ClusterBuilder\n",
    "from robyn.data.entities.enums import DependentVarType\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "cluster_configs = ClusteringConfig(\n",
    "    dep_var_type=DependentVarType(mmm_data.mmmdata_spec.dep_var_type),\n",
    "    cluster_by=ClusterBy.HYPERPARAMETERS,\n",
    "    max_clusters=10,\n",
    "    min_clusters=3,\n",
    "    weights=[1.0, 1.0, 1.0],\n",
    ")\n",
    "\n",
    "cluster_builder = ClusterBuilder(pareto_result=pareto_result)\n",
    "\n",
    "\n",
    "cluster_results = cluster_builder.cluster_models(cluster_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reestablish Pareto Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.modeling.pareto.pareto_utils import ParetoUtils\n",
    "\n",
    "utils = ParetoUtils()\n",
    "pareto_result = utils.process_pareto_clustered_results(\n",
    "    pareto_result,\n",
    "    clustered_result=cluster_results,\n",
    "    ran_cluster=True,\n",
    "    ran_calibration=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budget Allocation Optimization\n",
    "\n",
    "Robyn provides different scenarios for budget allocation optimization. Let's explore the \"max_response\" scenario:\n",
    "\n",
    "### Scenario: Maximum Response\n",
    "This scenario answers the question: \"What's the maximum return given certain spend constraints?\"\n",
    "\n",
    "Key parameters:\n",
    "- `total_budget`: When set to None, uses the total spend in the selected date range\n",
    "- `channel_constr_low`: Minimum spend multiplier (e.g., 0.7 means channel spend can't go below 70% of current)\n",
    "- `channel_constr_up`: Maximum spend multiplier per channel (e.g., 1.5 means channel spend can't exceed 150% of current)\n",
    "- `channel_constr_multiplier`: Extends bounds for wider optimization insights\n",
    "- `date_range`: Period for optimization (\"all\", \"last_X\", or specific date range)\n",
    "\n",
    "Note: Other scenarios include:\n",
    "- \"target_efficiency\": Optimize spend to hit specific ROAS or CPA targets\n",
    "- \"min_spend\": Find minimum spend required to hit response targets\n",
    "- \"max_response_expected\": Maximize expected response within confidence intervals\n",
    "\n",
    "For this example, we'll demonstrate the \"max_response\" scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model = pareto_result.pareto_solutions[0]\n",
    "\n",
    "print(select_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.allocator.entities.allocation_params import AllocatorParams\n",
    "from robyn.allocator.entities.allocation_result import (\n",
    "    AllocationResult,\n",
    "    OptimOutData,\n",
    "    MainPoints,\n",
    ")\n",
    "from robyn.allocator.entities.optimization_result import OptimizationResult\n",
    "from robyn.allocator.entities.constraints import Constraints\n",
    "from robyn.allocator.optimizer import BudgetAllocator\n",
    "from robyn.allocator.constants import (\n",
    "    SCENARIO_MAX_RESPONSE,\n",
    "    CONSTRAINT_MODE_EQ,\n",
    ")\n",
    "\n",
    "\n",
    "# Create allocator parameters matching R Example 1\n",
    "allocator_params = AllocatorParams(\n",
    "    scenario=SCENARIO_MAX_RESPONSE,\n",
    "    total_budget=None,  # When None, uses total spend in date_range\n",
    "    target_value=None,\n",
    "    date_range=\"all\",\n",
    "    channel_constr_low=[0.7],  # Single value for all channels\n",
    "    channel_constr_up=[1.2, 1.5, 1.5, 1.5, 1.5],  # Different values per channel\n",
    "    channel_constr_multiplier=3.0,\n",
    "    optim_algo=\"SLSQP_AUGLAG\",\n",
    "    maxeval=100000,\n",
    "    constr_mode=CONSTRAINT_MODE_EQ,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "# Initialize budget allocator\n",
    "max_response_allocator = BudgetAllocator(\n",
    "    mmm_data=mmm_data,\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    pareto_result=pareto_result,\n",
    "    select_model=select_model,\n",
    "    params=allocator_params,\n",
    ")\n",
    "\n",
    "## Step 3: Run Optimization\n",
    "max_response_result = max_response_allocator.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Create summary of optimization results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Channel\": max_response_result.dt_optimOut.channels,\n",
    "        \"Initial Spend\": max_response_result.dt_optimOut.init_spend_unit,\n",
    "        \"Optimized Spend\": max_response_result.dt_optimOut.optm_spend_unit,\n",
    "        \"Spend Change %\": (\n",
    "            max_response_result.dt_optimOut.optm_spend_unit\n",
    "            / max_response_result.dt_optimOut.init_spend_unit\n",
    "            - 1\n",
    "        )\n",
    "        * 100,\n",
    "        \"Initial Response\": max_response_result.dt_optimOut.init_response_unit,\n",
    "        \"Optimized Response\": max_response_result.dt_optimOut.optm_response_unit,\n",
    "        \"Response Lift %\": (\n",
    "            max_response_result.dt_optimOut.optm_response_unit\n",
    "            / max_response_result.dt_optimOut.init_response_unit\n",
    "            - 1\n",
    "        )\n",
    "        * 100,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budget Allocation Visualization Results\n",
    "\n",
    "The allocator generates two key plots to visualize optimization results:\n",
    "\n",
    "### 1. Budget Comparison Plot (`_plot_budget_comparison`)\n",
    "This plot shows a comparison between three scenarios: Initial, Bounded, and Bounded x3:\n",
    "- Bar pairs showing total spend vs total response for each scenario\n",
    "- Each scenario includes metrics text showing:\n",
    "  - For Initial: Spend, Response, and ROAS/CPA\n",
    "  - For others: Percent changes in Spend and Response, plus ROAS/CPA\n",
    "- Color-coded bars (Silver for Initial, Steel Blue for Bounded, Golden Rod for Bounded x3)\n",
    "- Values scaled up to represent the full analysis period\n",
    "\n",
    "### 2. Allocation Matrix Plot (`_plot_allocation_matrix`)\n",
    "Shows three side-by-side heatmaps comparing allocation strategies:\n",
    "- Columns in each heatmap represent:\n",
    "  - Absolute mean spend (in thousands)\n",
    "  - Mean spend percentage\n",
    "  - Mean response percentage\n",
    "  - Mean ROAS/CPA\n",
    "  - Marginal ROAS/CPA\n",
    "- Each row represents a different paid media channel\n",
    "- Color intensity indicates relative values within each scenario\n",
    "- Different color schemes for each scenario:\n",
    "  - Initial: Grey scale\n",
    "  - Bounded: Blue scale\n",
    "  - Bounded x3: Yellow/Gold scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.visualization.allocator_visualizer import AllocatorPlotter\n",
    "\n",
    "plotter = AllocatorPlotter(\n",
    "    allocation_result=max_response_result, budget_allocator=max_response_allocator\n",
    ")\n",
    "\n",
    "plots = plotter.plot_all(display_plots=True, export_location=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
