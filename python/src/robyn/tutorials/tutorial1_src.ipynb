{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robyn: Marketing Mix Modeling Application\n",
    "\n",
    "This notebook demonstrates the usage of Robyn, a Marketing Mix Modeling (MMM) application. \n",
    "We'll go through the main steps of performing robyn_inputs and robyn_engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Import Required Libraries. Define Paths.\n",
    "\n",
    "First, be sure to setup your virtual environment. Be sure to switch over to your new environment in this notebook. \n",
    "\n",
    "-```cd {root_folder}```\n",
    "\n",
    "-```python3 -m yourvenv```\n",
    "\n",
    "-```source yourvenv/bin/activate```\n",
    "\n",
    "-```cd Robyn/python```\n",
    "\n",
    "-```pip install -r requirements.txt```\n",
    "\n",
    "\n",
    "Then import the necessary libraries. Make sure to define your paths below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add Robyn to path\n",
    "sys.path.append(\"/Users/yijuilee/robynpy_release_reviews/Robyn/python/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.data.entities.enums import AdstockType\n",
    "from robyn.data.entities.holidays_data import HolidaysData\n",
    "from robyn.data.entities.hyperparameters import Hyperparameters, ChannelHyperparameters\n",
    "from robyn.modeling.entities.modelrun_trials_config import TrialsConfig\n",
    "from robyn.modeling.model_executor import ModelExecutor\n",
    "from robyn.modeling.entities.enums import NevergradAlgorithm, Models\n",
    "from robyn.modeling.feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Mock R data\n",
    "\n",
    "We need to set the base path for the data directory.\n",
    "Create a .env file in the same directory as your notebook and put in define the path to the data dir.\n",
    "for example: ROBYN_BASE_PATH=.../Robyn/R/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Data...\n",
      "Holidays Data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>holiday</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>AD</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-01-06</td>\n",
       "      <td>Epiphany</td>\n",
       "      <td>AD</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-02-28</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>AD</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-03-14</td>\n",
       "      <td>Constitution Day</td>\n",
       "      <td>AD</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995-04-14</td>\n",
       "      <td>Good Friday</td>\n",
       "      <td>AD</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds           holiday country  year\n",
       "0  1995-01-01    New Year's Day      AD  1995\n",
       "1  1995-01-06          Epiphany      AD  1995\n",
       "2  1995-02-28          Carnival      AD  1995\n",
       "3  1995-03-14  Constitution Day      AD  1995\n",
       "4  1995-04-14       Good Friday      AD  1995"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the simulated data and holidays data\n",
    "dt_simulated_weekly = pd.read_csv(\"resources/dt_simulated_weekly.csv\")\n",
    "\n",
    "print(\"Simulated Data...\")\n",
    "dt_simulated_weekly.head()\n",
    "\n",
    "dt_prophet_holidays = pd.read_csv(\"resources/dt_prophet_holidays.csv\")\n",
    "\n",
    "print(\"Holidays Data...\")\n",
    "dt_prophet_holidays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MMM Data\n",
    "\n",
    "We will now set up the MMM data specification which includes defining the dependent variable, independent variables, and the time window for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>revenue</th>\n",
       "      <th>tv_S</th>\n",
       "      <th>ooh_S</th>\n",
       "      <th>print_S</th>\n",
       "      <th>facebook_I</th>\n",
       "      <th>search_clicks_P</th>\n",
       "      <th>search_S</th>\n",
       "      <th>competitor_sales_B</th>\n",
       "      <th>facebook_S</th>\n",
       "      <th>events</th>\n",
       "      <th>newsletter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-23</td>\n",
       "      <td>2.754372e+06</td>\n",
       "      <td>22358.346667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12728.488889</td>\n",
       "      <td>2.430128e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8125009</td>\n",
       "      <td>7607.132915</td>\n",
       "      <td>na</td>\n",
       "      <td>19401.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-30</td>\n",
       "      <td>2.584277e+06</td>\n",
       "      <td>28613.453333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.527033e+06</td>\n",
       "      <td>9837.238486</td>\n",
       "      <td>4133.333333</td>\n",
       "      <td>7901549</td>\n",
       "      <td>1141.952450</td>\n",
       "      <td>na</td>\n",
       "      <td>14791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-07</td>\n",
       "      <td>2.547387e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132278.4</td>\n",
       "      <td>453.866667</td>\n",
       "      <td>1.665159e+07</td>\n",
       "      <td>12044.119653</td>\n",
       "      <td>3786.666667</td>\n",
       "      <td>8300197</td>\n",
       "      <td>4256.375378</td>\n",
       "      <td>na</td>\n",
       "      <td>14544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-14</td>\n",
       "      <td>2.875220e+06</td>\n",
       "      <td>83450.306667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17680.000000</td>\n",
       "      <td>1.054977e+07</td>\n",
       "      <td>12268.070319</td>\n",
       "      <td>4253.333333</td>\n",
       "      <td>8122883</td>\n",
       "      <td>2800.490677</td>\n",
       "      <td>na</td>\n",
       "      <td>2800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-21</td>\n",
       "      <td>2.215953e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277336.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.934090e+06</td>\n",
       "      <td>9467.248023</td>\n",
       "      <td>3613.333333</td>\n",
       "      <td>7105985</td>\n",
       "      <td>689.582605</td>\n",
       "      <td>na</td>\n",
       "      <td>15478.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE       revenue          tv_S     ooh_S       print_S  \\\n",
       "0  2015-11-23  2.754372e+06  22358.346667       0.0  12728.488889   \n",
       "1  2015-11-30  2.584277e+06  28613.453333       0.0      0.000000   \n",
       "2  2015-12-07  2.547387e+06      0.000000  132278.4    453.866667   \n",
       "3  2015-12-14  2.875220e+06  83450.306667       0.0  17680.000000   \n",
       "4  2015-12-21  2.215953e+06      0.000000  277336.0      0.000000   \n",
       "\n",
       "     facebook_I  search_clicks_P     search_S  competitor_sales_B  \\\n",
       "0  2.430128e+07         0.000000     0.000000             8125009   \n",
       "1  5.527033e+06      9837.238486  4133.333333             7901549   \n",
       "2  1.665159e+07     12044.119653  3786.666667             8300197   \n",
       "3  1.054977e+07     12268.070319  4253.333333             8122883   \n",
       "4  2.934090e+06      9467.248023  3613.333333             7105985   \n",
       "\n",
       "    facebook_S events    newsletter  \n",
       "0  7607.132915     na  19401.653846  \n",
       "1  1141.952450     na  14791.000000  \n",
       "2  4256.375378     na  14544.000000  \n",
       "3  2800.490677     na   2800.000000  \n",
       "4   689.582605     na  15478.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_mmm_data(dt_simulated_weekly) -> MMMData:\n",
    "\n",
    "    mmm_data_spec = MMMData.MMMDataSpec(\n",
    "        dep_var=\"revenue\",\n",
    "        dep_var_type=\"revenue\",\n",
    "        date_var=\"DATE\",\n",
    "        context_vars=[\"competitor_sales_B\", \"events\"],\n",
    "        paid_media_spends=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"],\n",
    "        paid_media_vars=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_I\", \"search_clicks_P\"],\n",
    "        organic_vars=[\"newsletter\"],\n",
    "        window_start=\"2016-01-01\",\n",
    "        window_end=\"2018-12-31\",\n",
    "    )\n",
    "\n",
    "    return MMMData(data=dt_simulated_weekly, mmmdata_spec=mmm_data_spec)\n",
    "\n",
    "\n",
    "mmm_data = setup_mmm_data(dt_simulated_weekly)\n",
    "mmm_data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing\n",
    "\n",
    "We will perform feature engineering to prepare the data for modeling. This includes transformations like adstock and other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters setup complete.\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    {\n",
    "        \"facebook_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"print_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"tv_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.3, 0.8],\n",
    "        ),\n",
    "        \"search_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"ooh_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"newsletter\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "    },\n",
    "    adstock=AdstockType.GEOMETRIC,\n",
    "    lambda_=0.0,\n",
    "    train_size=[0.5, 0.8],\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HolidaysData object\n",
    "holidays_data = HolidaysData(\n",
    "    dt_holidays=dt_prophet_holidays,\n",
    "    prophet_vars=[\"trend\", \"season\", \"holiday\"],\n",
    "    prophet_country=\"DE\",\n",
    "    prophet_signs=[\"default\", \"default\", \"default\"],\n",
    ")\n",
    "# Setup FeaturizedMMMData\n",
    "feature_engineering = FeatureEngineering(mmm_data, hyperparameters, holidays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:11:40 - robyn.modeling.feature_engineering - INFO - Starting feature engineering process\n",
      "2024-11-12 15:11:40 - robyn.modeling.feature_engineering - INFO - Starting Prophet decomposition\n",
      "2024-11-12 15:11:40 - robyn.modeling.feature_engineering - INFO - Starting Prophet decomposition\n",
      "2024-11-12 15:11:41 - robyn.modeling.feature_engineering - INFO - Fitting Prophet model without factor variables\n",
      "15:11:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "2024-11-12 15:11:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:11:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "2024-11-12 15:11:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "2024-11-12 15:11:41 - robyn.modeling.feature_engineering - INFO - Prophet decomposition completed successfully\n",
      "2024-11-12 15:11:41 - robyn.modeling.feature_engineering - INFO - Prophet decomposition complete\n",
      "2024-11-12 15:11:41 - robyn.modeling.feature_engineering - INFO - Starting model runs for paid media variables\n",
      "2024-11-12 15:11:41 - robyn.modeling.feature_engineering - INFO - Fitting spend-exposure model for tv_S\n",
      "2024-11-12 15:11:42 - robyn.modeling.feature_engineering - INFO - Selected linear model for tv_S (R² = 1.0000)\n",
      "2024-11-12 15:11:42 - robyn.modeling.feature_engineering - INFO - Fitting spend-exposure model for ooh_S\n",
      "2024-11-12 15:11:43 - robyn.modeling.feature_engineering - INFO - Selected linear model for ooh_S (R² = 1.0000)\n",
      "2024-11-12 15:11:43 - robyn.modeling.feature_engineering - INFO - Fitting spend-exposure model for print_S\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Selected linear model for print_S (R² = 1.0000)\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Fitting spend-exposure model for facebook_S\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Selected Michaelis-Menten model for facebook_S (R² = 0.9813)\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Fitting spend-exposure model for search_S\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Selected Michaelis-Menten model for search_S (R² = 0.9672)\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Completed model runs for 5 channels\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Feature engineering complete\n",
      "/Users/yijuilee/robynpy_release_reviews/Robyn/python/src/robyn/modeling/feature_engineering.py:82: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  dt_mod = dt_mod.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "2024-11-12 15:11:44 - robyn.modeling.feature_engineering - INFO - Filled 0 missing values\n"
     ]
    }
   ],
   "source": [
    "featurized_mmm_data = feature_engineering.perform_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:11:45 - robyn.visualization.feature_visualization - INFO - Initializing FeaturePlotter\n"
     ]
    }
   ],
   "source": [
    "from robyn.visualization.feature_visualization import FeaturePlotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a FeaturePlotter instance\n",
    "feature_plotter = FeaturePlotter(mmm_data, hyperparameters)\n",
    "\n",
    "# # Plot spend-exposure relationship for each channel\n",
    "# for channel in mmm_data.mmmdata_spec.paid_media_spends:\n",
    "#     try:\n",
    "#         fig = feature_plotter.plot_spend_exposure(featurized_mmm_data, channel)\n",
    "#         plt.show()\n",
    "#     except ValueError as e:\n",
    "#         print(f\"Skipping {channel}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:11:45 - robyn.modeling.base_model_executor - INFO - Initializing BaseModelExecutor\n",
      "2024-11-12 15:11:45 - robyn.modeling.model_executor - INFO - Starting model execution with model_name=Models.RIDGE\n",
      "2024-11-12 15:11:45 - robyn.modeling.base_model_executor - INFO - Input validation successful\n",
      "2024-11-12 15:11:45 - robyn.modeling.base_model_executor - INFO - Preparing hyperparameters\n",
      "2024-11-12 15:11:45 - robyn.modeling.base_model_executor - INFO - Completed hyperparameter preparation with 19 parameters to optimize\n",
      "2024-11-12 15:11:45 - robyn.modeling.model_executor - INFO - Initializing Ridge model builder\n",
      "2024-11-12 15:11:45 - robyn.modeling.model_executor - INFO - Building models with configured parameters\n",
      "2024-11-12 15:11:45 - robyn.modeling.ridge_model_builder - INFO - Collecting hyperparameters for optimization... {'prepared_hyperparameters': Hyperparameters(hyperparameters={'facebook_S': ChannelHyperparameters(thetas=[0, 0.3], shapes=None, scales=None, alphas=[0.5, 3], gammas=[0.3, 1], penalty=None), 'print_S': ChannelHyperparameters(thetas=[0.1, 0.4], shapes=None, scales=None, alphas=[0.5, 3], gammas=[0.3, 1], penalty=None), 'tv_S': ChannelHyperparameters(thetas=[0.3, 0.8], shapes=None, scales=None, alphas=[0.5, 3], gammas=[0.3, 1], penalty=None), 'search_S': ChannelHyperparameters(thetas=[0, 0.3], shapes=None, scales=None, alphas=[0.5, 3], gammas=[0.3, 1], penalty=None), 'ooh_S': ChannelHyperparameters(thetas=[0.1, 0.4], shapes=None, scales=None, alphas=[0.5, 3], gammas=[0.3, 1], penalty=None), 'newsletter': ChannelHyperparameters(thetas=[0.1, 0.4], shapes=None, scales=None, alphas=[0.5, 3], gammas=[0.3, 1], penalty=None)}, adstock=<AdstockType.GEOMETRIC: 'geometric'>, lambda_=0.0, train_size=[0.5, 0.8], hyper_bound_list_updated={}), 'hyper_to_optimize': {'facebook_S_thetas': [0, 0.3], 'facebook_S_alphas': [0.5, 3], 'facebook_S_gammas': [0.3, 1], 'print_S_thetas': [0.1, 0.4], 'print_S_alphas': [0.5, 3], 'print_S_gammas': [0.3, 1], 'tv_S_thetas': [0.3, 0.8], 'tv_S_alphas': [0.5, 3], 'tv_S_gammas': [0.3, 1], 'search_S_thetas': [0, 0.3], 'search_S_alphas': [0.5, 3], 'search_S_gammas': [0.3, 1], 'ooh_S_thetas': [0.1, 0.4], 'ooh_S_alphas': [0.5, 3], 'ooh_S_gammas': [0.3, 1], 'newsletter_thetas': [0.1, 0.4], 'newsletter_alphas': [0.5, 3], 'newsletter_gammas': [0.3, 1], 'train_size': [0.5, 0.8]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Starting 5 trials with 100 iterations each using TwoPointsDE nevergrad algorithm on x cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running trial 1 of total 5 trials: 100%|███████████████████████████████████\n",
      "2024-11-12 15:11:51 - robyn.modeling.ridge_model_builder - INFO -  Finished in 0.11 mins\n",
      "Running trial 2 of total 5 trials: 100%|███████████████████████████████████\n",
      "2024-11-12 15:11:58 - robyn.modeling.ridge_model_builder - INFO -  Finished in 0.11 mins\n",
      "Running trial 3 of total 5 trials: 100%|███████████████████████████████████\n",
      "2024-11-12 15:12:04 - robyn.modeling.ridge_model_builder - INFO -  Finished in 0.11 mins\n",
      "Running trial 4 of total 5 trials: 100%|███████████████████████████████████\n",
      "2024-11-12 15:12:11 - robyn.modeling.ridge_model_builder - INFO -  Finished in 0.10 mins\n",
      "Running trial 5 of total 5 trials: 100%|███████████████████████████████████\n",
      "2024-11-12 15:12:17 - robyn.modeling.ridge_model_builder - INFO -  Finished in 0.11 mins\n",
      "2024-11-12 15:12:17 - robyn.visualization.model_convergence_visualizer - INFO - Initialized ModelConvergenceVisualizer with n_cuts=20, nrmse_win=[0, 0.998]\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - INFO - Starting convergence calculation\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - WARNING - 'mape' column not found or all zeros. Assuming model is not calibrated.\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - WARNING - Convergence status for DECOMP.RSSD: NOT converged\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - INFO - DECOMP.RSSD NOT converged: sd@qt.20 43067.664 <= 50844.924 & |med@qt.20| 318034.75 > 268483.99\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - WARNING - Convergence status for NRMSE: NOT converged\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - INFO - NRMSE NOT converged: sd@qt.20 0.001 <= 0.001 & |med@qt.20| 0.06 > 0.05\n",
      "2024-11-12 15:12:17 - robyn.modeling.convergence.convergence - INFO - Convergence calculation completed successfully\n",
      "2024-11-12 15:12:17 - robyn.modeling.model_executor - INFO - Model building completed successfully\n",
      "2024-11-12 15:12:17 - robyn.modeling.model_executor - INFO - Model execution completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Setup ModelExecutor\n",
    "model_executor = ModelExecutor(\n",
    "    mmmdata=mmm_data,\n",
    "    holidays_data=holidays_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    calibration_input=None,  # Add calibration input if available\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    ")\n",
    "\n",
    "# Setup TrialsConfig\n",
    "trials_config = TrialsConfig(iterations=100, trials=5)  # Set to the number of cores you want to use\n",
    "\n",
    "print(\n",
    "    f\">>> Starting {trials_config.trials} trials with {trials_config.iterations} iterations each using {NevergradAlgorithm.TWO_POINTS_DE.value} nevergrad algorithm on x cores...\"\n",
    ")\n",
    "\n",
    "# Run the model\n",
    "\n",
    "output_models = model_executor.model_run(\n",
    "    trials_config=trials_config,\n",
    "    ts_validation=False,  # changed from True to False -> deacitvate\n",
    "    add_penalty_factor=False,\n",
    "    rssd_zero_penalty=True,\n",
    "    cores=8,\n",
    "    nevergrad_algo=NevergradAlgorithm.TWO_POINTS_DE,\n",
    "    intercept=True,\n",
    "    intercept_sign=\"non_negative\",\n",
    "    model_name=Models.RIDGE,\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# TODO need to fix graph outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model ID: 2_38_1\n"
     ]
    }
   ],
   "source": [
    "best_model_id = output_models.select_id\n",
    "print(f\"Best model ID: {best_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from robyn.modeling.pareto.pareto_optimizer import ParetoOptimizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create ParetoOptimizer instance\n",
    "pareto_optimizer = ParetoOptimizer(mmm_data, output_models, hyperparameters, featurized_mmm_data, holidays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:12:17 [INFO] Starting Pareto optimization\n",
      "2024-11-12 15:12:17 [INFO] Starting model data aggregation\n",
      "2024-11-12 15:12:17 [INFO] Computing Pareto fronts\n",
      "2024-11-12 15:12:17 [INFO] Pareto front computation completed\n",
      "2024-11-12 15:12:17 [INFO] Preparing Pareto data\n",
      "2024-11-12 15:12:17 [INFO] Number of Pareto-optimal solutions found: 500\n",
      "2024-11-12 15:12:17 [WARNING] Using all available Pareto fronts (5) with 5 total candidates\n",
      "/Users/yijuilee/robynpy_release_reviews/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py:356: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  pareto_fronts = int(auto_pareto[\"robynPareto\"])\n",
      "2024-11-12 15:12:17 [INFO] Filtering data for selected Pareto fronts...\n",
      "2024-11-12 15:12:17 [WARNING] No data in decomp_spend_dist. Skipping response curves calculation.\n",
      "2024-11-12 15:12:17 [INFO] Starting plot data generation...\n",
      "2024-11-12 15:12:17 [INFO] Processing Pareto front 1\n",
      "2024-11-12 15:12:18 [INFO] Pareto-Front: 1 [1 models]\n",
      "Processing Solutions:   0%|          | 0/1 [00:00<?, ?solution/s]2024-11-12 15:12:18 [ERROR] Error processing solution 3_62_1: \"The following id_vars or value_vars are not present in the DataFrame: ['spend_share', 'effect_share', 'roi_total', 'cpa_total']\"\n",
      "Processing Solutions:   0%|          | 0/1 [00:00<?, ?solution/s]\n",
      "2024-11-12 15:12:18 [ERROR] Error during Pareto optimization: \"The following id_vars or value_vars are not present in the DataFrame: ['spend_share', 'effect_share', 'roi_total', 'cpa_total']\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The following id_vars or value_vars are not present in the DataFrame: ['spend_share', 'effect_share', 'roi_total', 'cpa_total']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Run optimize function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pareto_result \u001b[38;5;241m=\u001b[39m \u001b[43mpareto_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpareto_fronts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/robynpy_release_reviews/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py:236\u001b[0m, in \u001b[0;36mParetoOptimizer.optimize\u001b[0;34m(self, pareto_fronts, min_candidates, calibration_constraint, calibrated)\u001b[0m\n\u001b[1;32m    232\u001b[0m pareto_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_pareto_data(\n\u001b[1;32m    233\u001b[0m     aggregated_data, pareto_fronts, min_candidates, calibrated\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m pareto_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_response_curves(pareto_data, aggregated_data)\n\u001b[0;32m--> 236\u001b[0m plotting_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_plot_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpareto_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPareto optimization completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ParetoResult(\n\u001b[1;32m    240\u001b[0m     pareto_solutions\u001b[38;5;241m=\u001b[39mplotting_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpareto_solutions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    241\u001b[0m     pareto_fronts\u001b[38;5;241m=\u001b[39mpareto_fronts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m     df_caov_pct_all\u001b[38;5;241m=\u001b[39mplotting_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_caov_pct_all\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    249\u001b[0m )\n",
      "File \u001b[0;32m~/robynpy_release_reviews/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py:1013\u001b[0m, in \u001b[0;36mParetoOptimizer._generate_plot_data\u001b[0;34m(self, aggregated_data, pareto_data)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing solution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1013\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1015\u001b[0m pareto_solutions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msol_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mediaVecCollect\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;66;03m# Update the set with unique sol_id values from the DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/robynpy_release_reviews/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py:623\u001b[0m, in \u001b[0;36mParetoOptimizer._generate_plot_data\u001b[0;34m(self, aggregated_data, pareto_data)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sid \u001b[38;5;129;01min\u001b[39;00m tqdm(uniqueSol, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Solutions\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolution\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;66;03m# 1. Spend x effect share comparison\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m         temp \u001b[38;5;241m=\u001b[39m \u001b[43mplotMediaShare\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplotMediaShare\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msol_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnrmse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecomp.rssd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrsq_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspend_share\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meffect_share\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpa_total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m         temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mCategorical(\n\u001b[1;32m    635\u001b[0m             temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrn\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    636\u001b[0m             categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmm_data\u001b[38;5;241m.\u001b[39mmmmdata_spec\u001b[38;5;241m.\u001b[39mpaid_media_spends),\n\u001b[1;32m    637\u001b[0m             ordered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    638\u001b[0m         )\n\u001b[1;32m    640\u001b[0m         plotMediaShareLoopBar \u001b[38;5;241m=\u001b[39m temp[\n\u001b[1;32m    641\u001b[0m             temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspend_share\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meffect_share\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    642\u001b[0m         ]\n",
      "File \u001b[0;32m~/robynpy_release_reviews/robynvenv/lib/python3.9/site-packages/pandas/core/frame.py:9942\u001b[0m, in \u001b[0;36mDataFrame.melt\u001b[0;34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m   9932\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaller\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf.melt(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   9933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelt\u001b[39m(\n\u001b[1;32m   9934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9940\u001b[0m     ignore_index: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   9941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m-> 9942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9943\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/robynpy_release_reviews/robynvenv/lib/python3.9/site-packages/pandas/core/reshape/melt.py:74\u001b[0m, in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     71\u001b[0m     missing_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     72\u001b[0m         lab \u001b[38;5;28;01mfor\u001b[39;00m lab, not_found \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, missing) \u001b[38;5;28;01mif\u001b[39;00m not_found\n\u001b[1;32m     73\u001b[0m     ]\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following id_vars or value_vars are not present in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value_vars_was_not_none:\n\u001b[1;32m     79\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39miloc[:, algos\u001b[38;5;241m.\u001b[39munique(idx)]\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The following id_vars or value_vars are not present in the DataFrame: ['spend_share', 'effect_share', 'roi_total', 'cpa_total']\""
     ]
    }
   ],
   "source": [
    "# 4. Run optimize function\n",
    "pareto_result = pareto_optimizer.optimize(pareto_fronts=\"auto\", min_candidates=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Check results\n",
    "print(\"Pareto Optimization Results:\")\n",
    "print(f\"Number of Pareto fronts: {pareto_result.pareto_solutions}\")\n",
    "print(f\"MediaVecCollect: {pareto_result.media_vec_collect.shape, pareto_result.media_vec_collect}\")\n",
    "print(\"\\Hyper parameter solutions:\")\n",
    "print(pareto_result.result_hyp_param)\n",
    "\n",
    "print(\"\\nAggregated decomposition results:\")\n",
    "print(pareto_result.x_decomp_agg)\n",
    "print(\"\\result Calibration:\")\n",
    "print(pareto_result.result_calibration)\n",
    "print(\"\\nx Decomp Vec Collect:\")\n",
    "print(pareto_result.x_decomp_vec_collect.shape, pareto_result.x_decomp_vec_collect)\n",
    "print(\"\\nCarryover percentage all:\")\n",
    "print(pareto_result.df_caov_pct_all.shape, pareto_result.df_caov_pct_all)\n",
    "print(\"\\Plot Data Collected\")\n",
    "print(\"NUMBER OF PLOTS Data collected for:\", len(pareto_result.plot_data_collect))\n",
    "print(\"Plot data for solid 5_221_9\", pareto_result.plot_data_collect)\n",
    "\n",
    "# 6. Validate logic\n",
    "assert pareto_result.pareto_fronts == \"auto\" or isinstance(\n",
    "    pareto_result.pareto_fronts, int\n",
    "), \"Invalid pareto_fronts value\"\n",
    "assert not pareto_result.result_hyp_param.empty, \"Empty result_hyp_param DataFrame\"\n",
    "assert not pareto_result.x_decomp_agg.empty, \"Empty x_decomp_agg DataFrame\"\n",
    "\n",
    "print(\"\\nAll assertions passed. The optimize function is working as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pareto_result.pareto_solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Setup and Import\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Union, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import necessary Robyn classes\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.modeling.entities.modeloutputs import ModelOutputs\n",
    "from robyn.data.entities.hyperparameters import Hyperparameters\n",
    "from robyn.allocator.entities.enums import OptimizationScenario, ConstrMode\n",
    "from robyn.allocator.budget_allocator import BudgetAllocator\n",
    "from robyn.modeling.pareto.pareto_optimizer import ParetoResult\n",
    "from robyn.allocator.entities.allocation_config import AllocationConfig\n",
    "from robyn.allocator.entities.allocation_constraints import AllocationConstraints\n",
    "from robyn.visualization.allocator_plotter import AllocationPlotter\n",
    "from utils.data_mapper import load_data_from_json, import_input_collect, import_output_collect, import_output_models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pareto_result.result_hyp_param[\"sol_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the correct model ID from your pareto results\n",
    "available_models = pareto_result.result_hyp_param[\"sol_id\"].unique()  # or 'solID' if that's the column name\n",
    "print(f\"Available models: {available_models}\")\n",
    "# Initialize allocator with a valid model ID\n",
    "select_model = available_models[0]  # Use first available model\n",
    "\n",
    "# Initialize budget allocator\n",
    "allocator = BudgetAllocator(\n",
    "    mmm_data=mmm_data,\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    "    model_outputs=output_models,\n",
    "    pareto_result=pareto_result,  # Get ParetoResult from import_output_collect()\n",
    "    select_model=select_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Different Optimization Scenarios\n",
    "\n",
    "### Scenario 1: Default Max Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base constraints matching R example\n",
    "channel_constraints = AllocationConstraints(\n",
    "    channel_constr_low={\n",
    "        \"tv_S\": 0.7,  # -30% from base\n",
    "        \"ooh_S\": 0.7,\n",
    "        \"print_S\": 0.7,\n",
    "        \"facebook_S\": 0.7,\n",
    "        \"search_S\": 0.7,\n",
    "    },\n",
    "    channel_constr_up={\n",
    "        \"tv_S\": 1.2,  # +20% from base\n",
    "        \"ooh_S\": 1.5,  # +50% from base\n",
    "        \"print_S\": 1.5,\n",
    "        \"facebook_S\": 1.5,\n",
    "        \"search_S\": 1.5,\n",
    "    },\n",
    "    channel_constr_multiplier=3.0,\n",
    ")\n",
    "\n",
    "# Configure max response scenario\n",
    "max_response_config = AllocationConfig(\n",
    "    scenario=OptimizationScenario.MAX_RESPONSE,\n",
    "    constraints=channel_constraints,\n",
    "    date_range=\"last\",  # Use last period as initial\n",
    "    total_budget=None,  # Use historical budget\n",
    "    maxeval=100000,\n",
    "    optim_algo=\"SLSQP_AUGLAG\",\n",
    "    constr_mode=ConstrMode.EQUALITY,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result = allocator.allocate(max_response_config)\n",
    "\n",
    "# Print results\n",
    "print(\n",
    "    f\"\"\"\n",
    "Model ID: {select_model}\n",
    "Scenario: {max_response_config.scenario}\n",
    "Use case: {result.metrics.get('use_case', '')}\n",
    "Window: {result.metrics.get('date_range_start')}:{result.metrics.get('date_range_end')} ({result.metrics.get('n_periods')} {mmm_data.mmmdata_spec.interval_type})\n",
    "\n",
    "Dep. Variable Type: {mmm_data.mmmdata_spec.dep_var_type}\n",
    "Media Skipped: {result.metrics.get('skipped_channels', 'None')}\n",
    "Relative Spend Increase: {result.metrics.get('spend_lift_pct', 0):.1f}% ({result.metrics.get('spend_lift_abs', 0):+.0f}K)\n",
    "Total Response Increase (Optimized): {result.metrics.get('response_lift', 0)*100:.1f}%\n",
    "\n",
    "Allocation Summary:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Print channel-level results\n",
    "for channel in mmm_data.mmmdata_spec.paid_media_spends:\n",
    "    current = result.optimal_allocations[result.optimal_allocations[\"channel\"] == channel].iloc[0]\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "- {channel}:\n",
    "  Optimizable bound: [{(current['constr_low']-1)*100:.0f}%, {(current['constr_up']-1)*100:.0f}%],\n",
    "  Initial spend share: {current['current_spend_share']*100:.2f}% -> Optimized bounded: {current['optimal_spend_share']*100:.2f}%\n",
    "  Initial response share: {current['current_response_share']*100:.2f}% -> Optimized bounded: {current['optimal_response_share']*100:.2f}%\n",
    "  Initial abs. mean spend: {current['current_spend']/1000:.3f}K -> Optimized: {current['optimal_spend']/1000:.3f}K [Delta = {(current['optimal_spend']/current['current_spend']-1)*100:.0f}%]\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Target Efficiency\n",
    "Optimize allocation based on target ROI/CPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scenario 3: Default Target Efficiency (Target ROAS or CPA)\n",
    "print(\"\\nScenario 3: Target efficiency optimization\")\n",
    "\n",
    "# Create constraints matching R's implementation\n",
    "default_channel_constraints = AllocationConstraints(\n",
    "    channel_constr_low={\n",
    "        channel: 0.1 for channel in mmm_data.mmmdata_spec.paid_media_spends  # -90% from base for all channels\n",
    "    },\n",
    "    channel_constr_up={\n",
    "        channel: 10.0 for channel in mmm_data.mmmdata_spec.paid_media_spends  # +900% from base for all channels\n",
    "    },\n",
    "    channel_constr_multiplier=1.0,  # Don't extend bounds for target efficiency\n",
    "    is_target_efficiency=True,  # Flag this as target efficiency scenario\n",
    ")\n",
    "\n",
    "# Create configuration for target efficiency scenario\n",
    "target_efficiency_config = AllocationConfig(\n",
    "    scenario=OptimizationScenario.TARGET_EFFICIENCY,\n",
    "    constraints=default_channel_constraints,\n",
    "    date_range=\"all\",  # Use all dates like in R version\n",
    "    target_value=None,  # Will use default 80% of initial ROAS or 120% of initial CPA\n",
    "    maxeval=100000,\n",
    "    optim_algo=\"SLSQP_AUGLAG\",\n",
    "    constr_mode=ConstrMode.EQUALITY,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result3 = allocator.allocate(target_efficiency_config)\n",
    "\n",
    "# Print results matching R format\n",
    "print(\n",
    "    f\"\"\"\n",
    "Model ID: {select_model}\n",
    "Scenario: {target_efficiency_config.scenario}\n",
    "Use case: {result3.metrics.get('use_case', '')}\n",
    "Window: {result3.metrics.get('date_range_start')}:{result3.metrics.get('date_range_end')} ({result3.metrics.get('n_periods')} {mmm_data.mmmdata_spec.interval_type})\n",
    "\n",
    "Dep. Variable Type: {mmm_data.mmmdata_spec.dep_var_type}\n",
    "Media Skipped: {result3.metrics.get('skipped_channels', 'None')}\n",
    "Relative Spend Increase: {result3.metrics.get('spend_lift_pct', 0):.0f}% ({result3.metrics.get('spend_lift_abs', 0):.0f})\n",
    "Total Response Increase (Optimized): {result3.metrics.get('response_lift', 0)*100:.0f}%\n",
    "\n",
    "Allocation Summary:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Print channel-level results\n",
    "for channel in mmm_data.mmmdata_spec.paid_media_spends:\n",
    "    current = result3.optimal_allocations[result3.optimal_allocations[\"channel\"] == channel].iloc[0]\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "- {channel}:\n",
    "  Optimizable bound: [{(current['constr_low']-1)*100:.0f}%, {(current['constr_up']-1)*100:.0f}%],\n",
    "  Initial spend share: {current['current_spend_share']*100:.2f}% -> Optimized bounded: {current['optimal_spend_share']*100:.2f}%\n",
    "  Initial response share: {current['current_response_share']*100:.2f}% -> Optimized bounded: {current['optimal_response_share']*100:.2f}%\n",
    "  Initial abs. mean spend: {current['current_spend']/1000:.3f}K -> Optimized: {current['optimal_spend']/1000:.3f}K [Delta = {(current['optimal_spend']/current['current_spend']-1)*100:.0f}%]\n",
    "\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for each scenario\n",
    "max_response_plotter = AllocationPlotter(result)\n",
    "target_efficiency_plotter = AllocationPlotter(result3)\n",
    "\n",
    "# Generate plots\n",
    "max_response_plots = max_response_plotter.plot_all()\n",
    "target_efficiency_plots = target_efficiency_plotter.plot_all()\n",
    "\n",
    "# Display plots\n",
    "print(\"Max Response Scenario Plots:\")\n",
    "print(\"-\" * 50)\n",
    "for plot_name, fig in max_response_plots.items():\n",
    "    display(fig)\n",
    "\n",
    "\n",
    "print(\"\\nTarget Efficiency Scenario Plots:\")\n",
    "print(\"-\" * 50)\n",
    "for plot_name, fig in target_efficiency_plots.items():\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
