{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robyn: Marketing Mix Modeling Application\n",
    "\n",
    "This notebook demonstrates the usage of Robyn, a Marketing Mix Modeling (MMM) application. \n",
    "We'll go through the main steps of performing robyn_inputs and robyn_engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Import Required Libraries. Define Paths.\n",
    "\n",
    "First, be sure to setup your virtual environment. Be sure to switch over to your new environment in this notebook. \n",
    "\n",
    "-```cd {root_folder}```\n",
    "\n",
    "-```python3 -m yourvenv```\n",
    "\n",
    "-```source yourvenv/bin/activate```\n",
    "\n",
    "-```cd Robyn/python```\n",
    "\n",
    "-```pip install -r requirements.txt```\n",
    "\n",
    "\n",
    "Then import the necessary libraries. Make sure to define your paths below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "base_path = \"/Users/yijuilee/project_robyn/robynpy_interfaces/Robyn/R/data\"\n",
    "python_path = \"/Users/yijuilee/robynpy_release_reviews/Robyn/python/src\"\n",
    "sys.path.append(base_path)\n",
    "sys.path.append(python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from typing import Dict, Any\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.data.entities.enums import AdstockType\n",
    "from robyn.data.entities.holidays_data import HolidaysData\n",
    "from robyn.data.entities.hyperparameters import Hyperparameters, ChannelHyperparameters\n",
    "from robyn.data.entities.calibration_input import CalibrationInput\n",
    "from robyn.modeling.entities.modelrun_trials_config import TrialsConfig\n",
    "from robyn.modeling.model_executor import ModelExecutor\n",
    "from robyn.modeling.ridge_model_builder import RidgeModelBuilder\n",
    "from robyn.modeling.entities.enums import NevergradAlgorithm, Models\n",
    "from robyn.modeling.feature_engineering import FeaturizedMMMData, FeatureEngineering\n",
    "from robyn.calibration.media_effect_calibration import MediaEffectCalibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Mock R data\n",
    "\n",
    "We need to set the base path for the data directory.\n",
    "Create a .env file in the same directory as your notebook and put in define the path to the data dir.\n",
    "for example: ROBYN_BASE_PATH=.../Robyn/R/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> Dict[str, pd.DataFrame]:\n",
    "    if not base_path:\n",
    "        raise EnvironmentError(\"Please set the ROBYN_BASE_PATH environment variable\")\n",
    "\n",
    "    simulated_weekly_path = os.path.join(base_path, \"dt_simulated_weekly.RData\")\n",
    "    prophet_holidays_path = os.path.join(base_path, \"dt_prophet_holidays.RData\")\n",
    "\n",
    "    result = pyreadr.read_r(simulated_weekly_path)\n",
    "    dt_simulated_weekly = result[\"dt_simulated_weekly\"]\n",
    "    result_holidays = pyreadr.read_r(prophet_holidays_path)\n",
    "    dt_prophet_holidays = result_holidays[\"dt_prophet_holidays\"]\n",
    "\n",
    "    return {\"dt_simulated_weekly\": dt_simulated_weekly, \"dt_prophet_holidays\": dt_prophet_holidays}\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "data[\"dt_simulated_weekly\"].head()\n",
    "data[\"dt_prophet_holidays\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MMM Data\n",
    "\n",
    "We will now set up the MMM data specification which includes defining the dependent variable, independent variables, and the time window for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mmm_data(data: Dict[str, pd.DataFrame]) -> MMMData:\n",
    "    dt_simulated_weekly = data[\"dt_simulated_weekly\"]\n",
    "\n",
    "    mmm_data_spec = MMMData.MMMDataSpec(\n",
    "        dep_var=\"revenue\",\n",
    "        dep_var_type=\"revenue\",\n",
    "        date_var=\"DATE\",\n",
    "        context_vars=[\"competitor_sales_B\", \"events\"],\n",
    "        paid_media_spends=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"],\n",
    "        paid_media_vars=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_I\", \"search_clicks_P\"],\n",
    "        organic_vars=[\"newsletter\"],\n",
    "        window_start=\"2016-01-01\",\n",
    "        window_end=\"2018-12-31\",\n",
    "    )\n",
    "\n",
    "    return MMMData(data=dt_simulated_weekly, mmmdata_spec=mmm_data_spec)\n",
    "\n",
    "\n",
    "mmm_data = setup_mmm_data(data)\n",
    "mmm_data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing\n",
    "\n",
    "We will perform feature engineering to prepare the data for modeling. This includes transformations like adstock and other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    {\n",
    "        \"facebook_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"print_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"tv_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.3, 0.8],\n",
    "        ),\n",
    "        \"search_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"ooh_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"newsletter\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "    },\n",
    "    adstock=AdstockType.GEOMETRIC,\n",
    "    lambda_=0.0,\n",
    "    train_size=[0.5, 0.8],\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HolidaysData object\n",
    "holidays_data = HolidaysData(\n",
    "    dt_holidays=data[\"dt_prophet_holidays\"],\n",
    "    prophet_vars=[\"trend\", \"season\", \"holiday\"],\n",
    "    prophet_country=\"DE\",\n",
    "    prophet_signs=[\"default\", \"default\", \"default\"],\n",
    ")\n",
    "# Setup FeaturizedMMMData\n",
    "feature_engineering = FeatureEngineering(mmm_data, hyperparameters, holidays_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Calibration Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.data.entities.enums import CalibrationScope, DependentVarType\n",
    "from robyn.data.entities.calibration_input import CalibrationInput, ChannelCalibrationData\n",
    "from robyn.calibration.media_effect_calibration import MediaEffectCalibrator\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample calibration data with explicit tuples\n",
    "channel_calibration_data = {\n",
    "    (\"facebook_S\"): ChannelCalibrationData(\n",
    "        lift_start_date=pd.Timestamp(\"2018-05-01\"),\n",
    "        lift_end_date=pd.Timestamp(\"2018-06-10\"),\n",
    "        lift_abs=400000.0,\n",
    "        spend=421000.0,\n",
    "        confidence=0.85,\n",
    "        metric=DependentVarType.REVENUE,\n",
    "        calibration_scope=CalibrationScope.IMMEDIATE,\n",
    "    ),\n",
    "    (\"tv_S\"): ChannelCalibrationData(\n",
    "        lift_start_date=pd.Timestamp(\"2018-04-03\"),\n",
    "        lift_end_date=pd.Timestamp(\"2018-06-03\"),\n",
    "        lift_abs=300000.0,\n",
    "        spend=7100.0,\n",
    "        confidence=0.8,\n",
    "        metric=DependentVarType.REVENUE,\n",
    "        calibration_scope=CalibrationScope.IMMEDIATE,\n",
    "    ),\n",
    "    (\"facebook_S\", \"search_S\"): ChannelCalibrationData(  # Tuple for combined channels\n",
    "        lift_start_date=pd.Timestamp(\"2018-07-01\"),\n",
    "        lift_end_date=pd.Timestamp(\"2018-07-20\"),\n",
    "        lift_abs=700000.0,\n",
    "        spend=350000.0,\n",
    "        confidence=0.99,\n",
    "        metric=DependentVarType.REVENUE,\n",
    "        calibration_scope=CalibrationScope.IMMEDIATE,\n",
    "    ),\n",
    "    (\"newsletter\"): ChannelCalibrationData(\n",
    "        lift_start_date=pd.Timestamp(\"2017-12-01\"),\n",
    "        lift_end_date=pd.Timestamp(\"2017-12-31\"),\n",
    "        lift_abs=200.0,\n",
    "        spend=0.0,\n",
    "        confidence=0.95,\n",
    "        metric=DependentVarType.REVENUE,\n",
    "        calibration_scope=CalibrationScope.IMMEDIATE,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Create the CalibrationInput object directly since keys are already tuples\n",
    "calibration_input = CalibrationInput(channel_data=channel_calibration_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_data = []\n",
    "for channels, data in calibration_input.channel_data.items():\n",
    "    df_data.append(\n",
    "        {\n",
    "            \"channel\": \"+\".join(channels),\n",
    "            \"lift_start_date\": data.lift_start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"lift_end_date\": data.lift_end_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"lift_abs\": f\"{data.lift_abs:,.0f}\",\n",
    "            \"spend\": f\"{data.spend:,.0f}\",\n",
    "            \"confidence\": f\"{data.confidence:.2f}\",\n",
    "            \"metric\": data.metric.value,\n",
    "            \"calibration_scope\": data.calibration_scope.value,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_calibration_input = pd.DataFrame(df_data)\n",
    "display(df_calibration_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "# Define the coefficient function locally in the notebook\n",
    "def get_model_coefficients(mmm_data: MMMData) -> Dict[str, float]:\n",
    "    \"\"\"Get approximate coefficients for channels based on data.\"\"\"\n",
    "    coefficients = {}\n",
    "    dep_var = mmm_data.mmmdata_spec.dep_var\n",
    "\n",
    "    for channel in mmm_data.mmmdata_spec.paid_media_spends + mmm_data.mmmdata_spec.organic_vars:\n",
    "        # Calculate simple correlation coefficient\n",
    "        corr = mmm_data.data[channel].corr(mmm_data.data[dep_var])\n",
    "        coefficients[channel] = abs(corr)  # Use absolute correlation as coefficient\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "\n",
    "# Configure logging to show debug messages\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Get model coefficients\n",
    "model_coefficients = get_model_coefficients(mmm_data)\n",
    "print(\"\\nEstimated model coefficients:\")\n",
    "for channel, coef in model_coefficients.items():\n",
    "    print(f\"{channel}: {coef:.4f}\")\n",
    "\n",
    "# Initialize calibration engine with coefficients\n",
    "calibration_engine = MediaEffectCalibrator(\n",
    "    mmm_data=mmm_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    calibration_input=calibration_input,\n",
    "    model_coefficients=model_coefficients,  # Add this parameter\n",
    ")\n",
    "\n",
    "# Perform calibration\n",
    "calibration_results = calibration_engine.calibrate()\n",
    "\n",
    "print(\"\\nCalibration Results:\")\n",
    "# Access channel scores specifically\n",
    "for channel, score in calibration_results.channel_scores.items():\n",
    "    if len(channel) == 1:\n",
    "        print(f\"{channel[0]}: MAPE = {score:.2%}\")\n",
    "    else:\n",
    "        print(f\"{' + '.join(channel)}: MAPE = {score:.2%}\")\n",
    "\n",
    "print(f\"\\nOverall Model Calibrated: {calibration_results.is_model_calibrated()}\")\n",
    "\n",
    "# Print detailed channel information\n",
    "print(\"\\nDetailed Channel Information:\")\n",
    "for channel_tuple, data in calibration_input.channel_data.items():\n",
    "    channels = channel_tuple if len(channel_tuple) > 1 else channel_tuple[0]\n",
    "    print(f\"\\nChannel: {channels}\")\n",
    "    print(f\"Lift Period: {data.lift_start_date} to {data.lift_end_date}\")\n",
    "    print(f\"Expected Lift: {data.lift_abs:,.2f}\")\n",
    "    print(f\"Spend: {data.spend:,.2f}\")\n",
    "    print(f\"Confidence: {data.confidence:.2%}\")\n",
    "\n",
    "    # Get actual values from data for this period\n",
    "    date_col = mmm_data.mmmdata_spec.date_var\n",
    "    mask = (mmm_data.data[date_col] >= data.lift_start_date) & (mmm_data.data[date_col] <= data.lift_end_date)\n",
    "\n",
    "    if isinstance(channels, tuple):\n",
    "        actual_values = sum(mmm_data.data.loc[mask, ch].sum() for ch in channels)\n",
    "    else:\n",
    "        actual_values = mmm_data.data.loc[mask, channels].sum()\n",
    "\n",
    "    print(f\"Actual Values Sum: {actual_values:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_mmm_data = feature_engineering.perform_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.visualization.feature_visualization import FeaturePlotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a FeaturePlotter instance\n",
    "feature_plotter = FeaturePlotter(mmm_data, hyperparameters)\n",
    "\n",
    "# Plot spend-exposure relationship for each channel\n",
    "for channel in mmm_data.mmmdata_spec.paid_media_spends:\n",
    "    try:\n",
    "        fig = feature_plotter.plot_spend_exposure(featurized_mmm_data, channel)\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {channel}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ModelExecutor\n",
    "model_executor = ModelExecutor(\n",
    "    mmmdata=mmm_data,\n",
    "    holidays_data=holidays_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    calibration_input=None,  # Add calibration input if available\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    ")\n",
    "\n",
    "# Setup TrialsConfig\n",
    "trials_config = TrialsConfig(iterations=2000, trials=5)  # Set to the number of cores you want to use\n",
    "\n",
    "print(\n",
    "    f\">>> Starting {trials_config.trials} trials with {trials_config.iterations} iterations each using {NevergradAlgorithm.TWO_POINTS_DE.value} nevergrad algorithm on x cores...\"\n",
    ")\n",
    "\n",
    "# Run the model\n",
    "\n",
    "output_models = model_executor.model_run(\n",
    "    trials_config=trials_config,\n",
    "    ts_validation=False,  # changed from True to False -> deacitvate\n",
    "    add_penalty_factor=False,\n",
    "    rssd_zero_penalty=True,\n",
    "    cores=8,\n",
    "    nevergrad_algo=NevergradAlgorithm.TWO_POINTS_DE,\n",
    "    intercept=True,\n",
    "    intercept_sign=\"non_negative\",\n",
    "    model_name=Models.RIDGE,\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# TODO fix graph outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model_outputs.trials[0] is already an object from your model\n",
    "trial = output_models.trials[0]\n",
    "\n",
    "\n",
    "# Function to check if an object has a 'shape' attribute\n",
    "def has_shape(obj):\n",
    "    return hasattr(obj, \"shape\")\n",
    "\n",
    "\n",
    "# Get all attribute names of the object and print their shapes if they have a 'shape' attribute\n",
    "attribute_names = [attr for attr in dir(trial) if not callable(getattr(trial, attr)) and not attr.startswith(\"__\")]\n",
    "for attribute_name in attribute_names:\n",
    "    attribute_value = getattr(trial, attribute_name)\n",
    "    if has_shape(attribute_value):\n",
    "        print(f\"{attribute_name}: Shape = {attribute_value.shape}\")\n",
    "    else:\n",
    "        print(f\"{attribute_name}: No shape attribute, Type = {type(attribute_value).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model_outputs.trials[0] is already an object from your model\n",
    "trial = output_models.trials[0]\n",
    "\n",
    "\n",
    "# Function to check if an object has a 'shape' attribute\n",
    "def has_shape(obj):\n",
    "    return hasattr(obj, \"shape\")\n",
    "\n",
    "\n",
    "# Get all attribute names of the object and print their shapes if they have a 'shape' attribute\n",
    "attribute_names = [attr for attr in dir(trial) if not callable(getattr(trial, attr)) and not attr.startswith(\"__\")]\n",
    "for attribute_name in attribute_names:\n",
    "    attribute_value = getattr(trial, attribute_name)\n",
    "    if has_shape(attribute_value):\n",
    "        print(f\"{attribute_name}: Shape = {attribute_value.shape}\")\n",
    "        # Check if the attribute is a multi-dimensional array with more than one column\n",
    "        if len(attribute_value.shape) > 1 and attribute_value.shape[1] > 1:\n",
    "            try:\n",
    "                # Attempt to print column names if it's a structured array or DataFrame\n",
    "                columns = (\n",
    "                    attribute_value.columns if hasattr(attribute_value, \"columns\") else attribute_value.dtype.names\n",
    "                )\n",
    "                print(f\"  Columns: {columns}\")\n",
    "            except AttributeError:\n",
    "                print(\"  No column names available.\")\n",
    "    else:\n",
    "        print(f\"{attribute_name}: No shape attribute, Type = {type(attribute_value).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_id = output_models.select_id\n",
    "print(f\"Best model ID: {best_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Display the MOO Distribution Plot\n",
    "if \"moo_distrb_plot\" in output_models.convergence:\n",
    "    moo_distrb_plot = output_models.convergence[\"moo_distrb_plot\"]\n",
    "    display(Image(data=base64.b64decode(moo_distrb_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Display the MOO Cloud Plot\n",
    "if \"moo_cloud_plot\" in output_models.convergence:\n",
    "    moo_cloud_plot = output_models.convergence[\"moo_cloud_plot\"]\n",
    "    display(Image(data=base64.b64decode(moo_cloud_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Print convergence messages\n",
    "if \"conv_msg\" in output_models.convergence:\n",
    "    for msg in output_models.convergence[\"conv_msg\"]:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Display time series validation and convergence plots\n",
    "if \"ts_validation_plot\" in output_models.convergence:\n",
    "    ts_validation_plot = output_models.convergence[\"ts_validation_plot\"]\n",
    "    display(Image(data=base64.b64decode(ts_validation_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_id = output_models.select_id\n",
    "print(f\"Best model ID: {best_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
