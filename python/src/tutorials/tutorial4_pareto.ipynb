{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Pareto Optimizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.modeling.entities.modeloutputs import ModelOutputs, Trial\n",
    "from robyn.modeling.pareto.pareto_optimizer import ParetoOptimizer\n",
    "from robyn.data.entities.enums import DependentVarType, PaidMediaSigns, OrganicSigns, ContextSigns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create dummy data for MMMData\n",
    "\n",
    "# Generate date range\n",
    "date_range = pd.date_range(start='2022-01-01', end='2022-12-31', freq='D')\n",
    "\n",
    "# Create dummy data\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'date': date_range,\n",
    "    'sales': np.random.randint(1000, 2000, size=len(date_range)),\n",
    "    'tv_spend': np.random.randint(100, 500, size=len(date_range)),\n",
    "    'radio_spend': np.random.randint(50, 200, size=len(date_range)),\n",
    "    'social_media_spend': np.random.randint(200, 600, size=len(date_range)),\n",
    "    'competitor_sales': np.random.randint(800, 1500, size=len(date_range)),\n",
    "    'holiday': np.random.choice([0, 1], size=len(date_range), p=[0.9, 0.1])\n",
    "})\n",
    "\n",
    "# Create MMMDataSpec\n",
    "mmmdata_spec = MMMData.MMMDataSpec(\n",
    "    dep_var='sales',\n",
    "    dep_var_type=DependentVarType.REVENUE,\n",
    "    date_var='date',\n",
    "    paid_media_spends=['tv_spend', 'radio_spend', 'social_media_spend'],\n",
    "    paid_media_vars=None,\n",
    "    paid_media_signs=[PaidMediaSigns.POSITIVE] * 3,\n",
    "    organic_vars=['competitor_sales'],\n",
    "    organic_signs=[OrganicSigns.NEGATIVE],\n",
    "    context_vars=['holiday'],\n",
    "    context_signs=[ContextSigns.POSITIVE],\n",
    "    factor_vars=None,\n",
    "    window_start=datetime(2022, 1, 1),\n",
    "    window_end=datetime(2022, 12, 31)\n",
    ")\n",
    "\n",
    "# Create MMMData instance\n",
    "mmm_data = MMMData(data, mmmdata_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Create dummy ModelOutputs\n",
    "\n",
    "def create_dummy_trial(trial_num):\n",
    "    num_solutions = 10  # Increase the number of solutions per trial\n",
    "    media_channels = ['tv_spend', 'radio_spend', 'social_media_spend', 'search_spend', 'print_spend']\n",
    "    \n",
    "    result_hyp_param = pd.DataFrame({\n",
    "        'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions)],\n",
    "        'nrmse': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "        'nrmse_train': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "        'nrmse_test': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "        'decomp.rssd': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "        'mape': np.random.uniform(5, 15, num_solutions),\n",
    "        'rsq_train': np.random.uniform(0.7, 0.9, num_solutions),\n",
    "        'rsq_val': np.random.uniform(0.6, 0.8, num_solutions),\n",
    "        'rsq_test': np.random.uniform(0.5, 0.7, num_solutions),\n",
    "        'iterNG': [1] * num_solutions,\n",
    "        'iterPar': [trial_num] * num_solutions,\n",
    "        'lambda': np.random.uniform(0.01, 0.1, num_solutions),\n",
    "        'lambda_hp': np.random.uniform(0.01, 0.1, num_solutions),\n",
    "        'lambda_max': np.random.uniform(0.1, 0.5, num_solutions),\n",
    "        'lambda_min_ratio': np.random.uniform(0.01, 0.1, num_solutions)\n",
    "    })\n",
    "    \n",
    "    # Ensure some solutions are Pareto-optimal\n",
    "    result_hyp_param['nrmse'] = np.sort(result_hyp_param['nrmse'])\n",
    "    result_hyp_param['decomp.rssd'] = np.sort(result_hyp_param['decomp.rssd'])[::-1]\n",
    "    \n",
    "    x_decomp_agg = pd.DataFrame({\n",
    "        'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions) for _ in media_channels],\n",
    "        'rn': media_channels * num_solutions,\n",
    "        'coef': np.random.uniform(0.1, 0.5, num_solutions * len(media_channels)),\n",
    "        'boot_mean': np.random.uniform(0.1, 0.5, num_solutions * len(media_channels)),\n",
    "        'iterNG': [1] * (num_solutions * len(media_channels)),\n",
    "        'iterPar': [trial_num] * (num_solutions * len(media_channels))\n",
    "    })\n",
    "    \n",
    "    decomp_spend_dist = pd.DataFrame({\n",
    "        'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions) for _ in media_channels],\n",
    "        'rn': media_channels * num_solutions,\n",
    "        'mean_spend': np.random.randint(100, 500, num_solutions * len(media_channels)),\n",
    "        'total_spend': np.random.randint(10000, 50000, num_solutions * len(media_channels)),\n",
    "        'xDecompAgg': np.random.randint(5000, 20000, num_solutions * len(media_channels)),\n",
    "        'spend_share': np.random.uniform(0, 1, num_solutions * len(media_channels)),\n",
    "        'effect_share': np.random.uniform(0, 1, num_solutions * len(media_channels)),\n",
    "        'iterNG': [1] * (num_solutions * len(media_channels)),\n",
    "        'iterPar': [trial_num] * (num_solutions * len(media_channels))\n",
    "    })\n",
    "    \n",
    "    lift_calibration = pd.DataFrame({\n",
    "        'liftMedia': media_channels * num_solutions,\n",
    "        'lift': np.random.uniform(1, 2, num_solutions * len(media_channels)),\n",
    "        'iterNG': [1] * (num_solutions * len(media_channels)),\n",
    "        'iterPar': [trial_num] * (num_solutions * len(media_channels)),\n",
    "        'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions) for _ in media_channels]\n",
    "    }) if trial_num % 2 == 0 else None\n",
    "    \n",
    "    return Trial(\n",
    "        result_hyp_param=result_hyp_param,\n",
    "        x_decomp_agg=x_decomp_agg,\n",
    "        decomp_spend_dist=decomp_spend_dist,\n",
    "        lift_calibration=lift_calibration,\n",
    "        nrmse=result_hyp_param['nrmse'].values[0],\n",
    "        decomp_rssd=result_hyp_param['decomp.rssd'].values[0],\n",
    "        mape=result_hyp_param['mape'].values[0],\n",
    "        rsq_train=result_hyp_param['rsq_train'].values[0],\n",
    "        rsq_val=result_hyp_param['rsq_val'].values[0],\n",
    "        rsq_test=result_hyp_param['rsq_test'].values[0],\n",
    "        lambda_=result_hyp_param['lambda'].values[0],\n",
    "        lambda_hp=result_hyp_param['lambda_hp'].values[0],\n",
    "        lambda_max=result_hyp_param['lambda_max'].values[0],\n",
    "        lambda_min_ratio=result_hyp_param['lambda_min_ratio'].values[0],\n",
    "        pos=trial_num,\n",
    "        elapsed=np.random.uniform(10, 30),\n",
    "        elapsed_accum=np.random.uniform(100, 300),\n",
    "        trial=trial_num,\n",
    "        iter_ng=1,\n",
    "        iter_par=trial_num,\n",
    "        train_size=0.7,\n",
    "        sol_id=f'sol_{trial_num}_0'\n",
    "    )\n",
    "\n",
    "# Create more trials\n",
    "trials = [create_dummy_trial(i) for i in range(1)]  # Increase the number of trials\n",
    "\n",
    "model_outputs = ModelOutputs(\n",
    "    trials=trials,\n",
    "    train_timestamp='2023-05-01 10:00:00',\n",
    "    cores=1,\n",
    "    iterations=10,\n",
    "    intercept=True,\n",
    "    intercept_sign='positive',\n",
    "    nevergrad_algo='NGOpt',\n",
    "    ts_validation=True,\n",
    "    add_penalty_factor=True,\n",
    "    hyper_updated={},\n",
    "    hyper_fixed=False,\n",
    "    convergence={},\n",
    "    select_id='sol_5',\n",
    "    seed=42,\n",
    "    hyper_bound_ng={},\n",
    "    hyper_bound_fixed={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Automatically selected 1 Pareto-fronts to contain at least 1 pareto-optimal models (1)\n",
      ">>> Calculating response curves for all models' media variables (0)...\n",
      "resp_collect Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "pareto_data.decomp_spend_dist Empty DataFrame\n",
      "Columns: [solID, rn, mean_spend, total_spend, xDecompAgg, spend_share, effect_share, iterNG, iterPar, trial, robynPareto]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/funny/Documents/git/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py:262: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pareto_fronts_df = pd.concat([pareto_fronts_df, pareto_front])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'solID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6t/fx01ty1n5lbcsyy0767wdgdw0000gn/T/ipykernel_36154/2306154585.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3. Create ParetoOptimizer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpareto_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParetoOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmm_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 4. Run optimize function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpareto_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpareto_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpareto_fronts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_candidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/git/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, pareto_fronts, min_candidates, calibration_constraint, calibrated)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0maggregated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibrated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0maggregated_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result_hyp_param'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_pareto_fronts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpareto_fronts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibration_constraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mpareto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_pareto_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpareto_fronts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mresponse_curves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_response_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpareto_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_curves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         return ParetoResult(\n",
      "\u001b[0;32m~/Documents/git/Robyn/python/src/robyn/modeling/pareto/pareto_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, pareto_data)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resp_collect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp_collect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pareto_data.decomp_spend_dist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpareto_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomp_spend_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;31m# Merge results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         decomp_spend_dist = pd.merge(\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0mpareto_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomp_spend_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mresp_collect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'solID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/Robyn/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/Robyn/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/Robyn/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/Robyn/.venv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'solID'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Create ParetoOptimizer instance\n",
    "pareto_optimizer = ParetoOptimizer(mmm_data, model_outputs)\n",
    "\n",
    "# 4. Run optimize function\n",
    "pareto_result = pareto_optimizer.optimize(pareto_fronts=\"auto\", min_candidates=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Check results\n",
    "print(\"Pareto Optimization Results:\")\n",
    "print(f\"Number of Pareto solutions: {len(pareto_result.pareto_solutions)}\")\n",
    "print(f\"Number of Pareto fronts: {pareto_result.pareto_fronts}\")\n",
    "print(\"\\nPareto-optimal solutions:\")\n",
    "print(pareto_result.result_hyp_param[['solID', 'nrmse', 'decomp.rssd', 'mape', 'robynPareto']])\n",
    "\n",
    "print(\"\\nAggregated decomposition results:\")\n",
    "print(pareto_result.x_decomp_agg.head())\n",
    "\n",
    "print(\"\\nMedia vector collection:\")\n",
    "print(pareto_result.media_vec_collect.head())\n",
    "\n",
    "print(\"\\nDecomposition vector collection:\")\n",
    "print(pareto_result.x_decomp_vec_collect.head())\n",
    "\n",
    "# 6. Validate logic\n",
    "assert len(pareto_result.pareto_solutions) > 0, \"No Pareto-optimal solutions found\"\n",
    "assert pareto_result.pareto_fronts > 0, \"Invalid number of Pareto fronts\"\n",
    "assert not pareto_result.result_hyp_param.empty, \"Empty result_hyp_param DataFrame\"\n",
    "assert not pareto_result.x_decomp_agg.empty, \"Empty x_decomp_agg DataFrame\"\n",
    "assert not pareto_result.media_vec_collect.empty, \"Empty media_vec_collect DataFrame\"\n",
    "assert not pareto_result.x_decomp_vec_collect.empty, \"Empty x_decomp_vec_collect DataFrame\"\n",
    "\n",
    "print(\"\\nAll assertions passed. The optimize function is working as expected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
