{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/funny/Documents/git/Robyn/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Test Pareto Optimizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.modeling.entities.modeloutputs import ModelOutputs, Trial\n",
    "from robyn.modeling.pareto.pareto_optimizer import ParetoOptimizer\n",
    "from robyn.data.entities.enums import DependentVarType, PaidMediaSigns, OrganicSigns, ContextSigns\n",
    "\n",
    "from utils.data_mapper import export_data, import_data, save_data_to_json, load_data_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. Create dummy data for MMMData\n",
    "\n",
    "# # Generate date range\n",
    "# date_range = pd.date_range(start='2022-01-01', end='2022-12-31', freq='D')\n",
    "\n",
    "# # Create dummy data\n",
    "# np.random.seed(42)\n",
    "# data = pd.DataFrame({\n",
    "#     'date': date_range,\n",
    "#     'sales': np.random.randint(1000, 2000, size=len(date_range)),\n",
    "#     'tv_spend': np.random.randint(100, 500, size=len(date_range)),\n",
    "#     'radio_spend': np.random.randint(50, 200, size=len(date_range)),\n",
    "#     'social_media_spend': np.random.randint(200, 600, size=len(date_range)),\n",
    "#     'competitor_sales': np.random.randint(800, 1500, size=len(date_range)),\n",
    "#     'holiday': np.random.choice([0, 1], size=len(date_range), p=[0.9, 0.1])\n",
    "# })\n",
    "\n",
    "# # Create MMMDataSpec\n",
    "# mmmdata_spec = MMMData.MMMDataSpec(\n",
    "#     dep_var='sales',\n",
    "#     dep_var_type=DependentVarType.REVENUE,\n",
    "#     date_var='date',\n",
    "#     paid_media_spends=['tv_spend', 'radio_spend', 'social_media_spend'],\n",
    "#     paid_media_vars=None,\n",
    "#     paid_media_signs=[PaidMediaSigns.POSITIVE] * 3,\n",
    "#     organic_vars=['competitor_sales'],\n",
    "#     organic_signs=[OrganicSigns.NEGATIVE],\n",
    "#     context_vars=['holiday'],\n",
    "#     context_signs=[ContextSigns.POSITIVE],\n",
    "#     factor_vars=None,\n",
    "#     window_start=datetime(2022, 1, 1),\n",
    "#     window_end=datetime(2022, 12, 31)\n",
    "# )\n",
    "\n",
    "# # Create MMMData instance\n",
    "# mmm_data = MMMData(data, mmmdata_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 2. Create dummy ModelOutputs\n",
    "\n",
    "# def create_dummy_trial(trial_num):\n",
    "#     num_solutions = 10  # Increase the number of solutions per trial\n",
    "#     media_channels = ['tv_spend', 'radio_spend', 'social_media_spend', 'search_spend', 'print_spend']\n",
    "    \n",
    "#     result_hyp_param = pd.DataFrame({\n",
    "#         'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions)],\n",
    "#         'nrmse': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "#         'nrmse_train': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "#         'nrmse_test': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "#         'decomp.rssd': np.random.uniform(0.1, 0.3, num_solutions),\n",
    "#         'mape': np.random.uniform(5, 15, num_solutions),\n",
    "#         'rsq_train': np.random.uniform(0.7, 0.9, num_solutions),\n",
    "#         'rsq_val': np.random.uniform(0.6, 0.8, num_solutions),\n",
    "#         'rsq_test': np.random.uniform(0.5, 0.7, num_solutions),\n",
    "#         'iterNG': [1] * num_solutions,\n",
    "#         'iterPar': [trial_num] * num_solutions,\n",
    "#         'lambda': np.random.uniform(0.01, 0.1, num_solutions),\n",
    "#         'lambda_hp': np.random.uniform(0.01, 0.1, num_solutions),\n",
    "#         'lambda_max': np.random.uniform(0.1, 0.5, num_solutions),\n",
    "#         'lambda_min_ratio': np.random.uniform(0.01, 0.1, num_solutions)\n",
    "#     })\n",
    "    \n",
    "#     # Ensure some solutions are Pareto-optimal\n",
    "#     result_hyp_param['nrmse'] = np.sort(result_hyp_param['nrmse'])\n",
    "#     result_hyp_param['decomp.rssd'] = np.sort(result_hyp_param['decomp.rssd'])[::-1]\n",
    "    \n",
    "#     x_decomp_agg = pd.DataFrame({\n",
    "#         'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions) for _ in media_channels],\n",
    "#         'rn': media_channels * num_solutions,\n",
    "#         'coef': np.random.uniform(0.1, 0.5, num_solutions * len(media_channels)),\n",
    "#         'boot_mean': np.random.uniform(0.1, 0.5, num_solutions * len(media_channels)),\n",
    "#         'iterNG': [1] * (num_solutions * len(media_channels)),\n",
    "#         'iterPar': [trial_num] * (num_solutions * len(media_channels))\n",
    "#     })\n",
    "    \n",
    "#     decomp_spend_dist = pd.DataFrame({\n",
    "#         'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions) for _ in media_channels],\n",
    "#         'rn': media_channels * num_solutions,\n",
    "#         'mean_spend': np.random.randint(100, 500, num_solutions * len(media_channels)),\n",
    "#         'total_spend': np.random.randint(10000, 50000, num_solutions * len(media_channels)),\n",
    "#         'xDecompAgg': np.random.randint(5000, 20000, num_solutions * len(media_channels)),\n",
    "#         'spend_share': np.random.uniform(0, 1, num_solutions * len(media_channels)),\n",
    "#         'effect_share': np.random.uniform(0, 1, num_solutions * len(media_channels)),\n",
    "#         'iterNG': [1] * (num_solutions * len(media_channels)),\n",
    "#         'iterPar': [trial_num] * (num_solutions * len(media_channels))\n",
    "#     })\n",
    "    \n",
    "#     lift_calibration = pd.DataFrame({\n",
    "#         'liftMedia': media_channels * num_solutions,\n",
    "#         'lift': np.random.uniform(1, 2, num_solutions * len(media_channels)),\n",
    "#         'iterNG': [1] * (num_solutions * len(media_channels)),\n",
    "#         'iterPar': [trial_num] * (num_solutions * len(media_channels)),\n",
    "#         'solID': [f'sol_{trial_num}_{i}' for i in range(num_solutions) for _ in media_channels]\n",
    "#     }) if trial_num % 2 == 0 else None\n",
    "    \n",
    "#     return Trial(\n",
    "#         result_hyp_param=result_hyp_param,\n",
    "#         x_decomp_agg=x_decomp_agg,\n",
    "#         decomp_spend_dist=decomp_spend_dist,\n",
    "#         lift_calibration=lift_calibration,\n",
    "#         nrmse=result_hyp_param['nrmse'].values[0],\n",
    "#         decomp_rssd=result_hyp_param['decomp.rssd'].values[0],\n",
    "#         mape=result_hyp_param['mape'].values[0],\n",
    "#         rsq_train=result_hyp_param['rsq_train'].values[0],\n",
    "#         rsq_val=result_hyp_param['rsq_val'].values[0],\n",
    "#         rsq_test=result_hyp_param['rsq_test'].values[0],\n",
    "#         lambda_=result_hyp_param['lambda'].values[0],\n",
    "#         lambda_hp=result_hyp_param['lambda_hp'].values[0],\n",
    "#         lambda_max=result_hyp_param['lambda_max'].values[0],\n",
    "#         lambda_min_ratio=result_hyp_param['lambda_min_ratio'].values[0],\n",
    "#         pos=trial_num,\n",
    "#         elapsed=np.random.uniform(10, 30),\n",
    "#         elapsed_accum=np.random.uniform(100, 300),\n",
    "#         trial=trial_num,\n",
    "#         iter_ng=1,\n",
    "#         iter_par=trial_num,\n",
    "#         train_size=0.7,\n",
    "#         sol_id=f'sol_{trial_num}_0'\n",
    "#     )\n",
    "\n",
    "# # Create more trials\n",
    "# trials = [create_dummy_trial(i) for i in range(1)]  # Increase the number of trials\n",
    "\n",
    "# model_outputs = ModelOutputs(\n",
    "#     trials=trials,\n",
    "#     train_timestamp='2023-05-01 10:00:00',\n",
    "#     cores=1,\n",
    "#     iterations=10,\n",
    "#     intercept=True,\n",
    "#     intercept_sign='positive',\n",
    "#     nevergrad_algo='NGOpt',\n",
    "#     ts_validation=True,\n",
    "#     add_penalty_factor=True,\n",
    "#     hyper_updated={},\n",
    "#     hyper_fixed=False,\n",
    "#     convergence={},\n",
    "#     select_id='sol_5',\n",
    "#     seed=42,\n",
    "#     hyper_bound_ng={},\n",
    "#     hyper_bound_fixed={}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facebook_S_alphas</th>\n",
       "      <th>facebook_S_gammas</th>\n",
       "      <th>facebook_S_thetas</th>\n",
       "      <th>newsletter_alphas</th>\n",
       "      <th>newsletter_gammas</th>\n",
       "      <th>newsletter_thetas</th>\n",
       "      <th>ooh_S_alphas</th>\n",
       "      <th>ooh_S_gammas</th>\n",
       "      <th>ooh_S_thetas</th>\n",
       "      <th>print_S_alphas</th>\n",
       "      <th>print_S_gammas</th>\n",
       "      <th>print_S_thetas</th>\n",
       "      <th>search_S_alphas</th>\n",
       "      <th>search_S_gammas</th>\n",
       "      <th>search_S_thetas</th>\n",
       "      <th>tv_S_alphas</th>\n",
       "      <th>tv_S_gammas</th>\n",
       "      <th>tv_S_thetas</th>\n",
       "      <th>train_size</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   facebook_S_alphas  facebook_S_gammas  facebook_S_thetas  newsletter_alphas  \\\n",
       "0                0.5                0.3                0.0                0.5   \n",
       "1                3.0                1.0                0.3                3.0   \n",
       "\n",
       "   newsletter_gammas  newsletter_thetas  ooh_S_alphas  ooh_S_gammas  \\\n",
       "0                0.3                0.1           0.5           0.3   \n",
       "1                1.0                0.4           3.0           1.0   \n",
       "\n",
       "   ooh_S_thetas  print_S_alphas  print_S_gammas  print_S_thetas  \\\n",
       "0           0.1             0.5             0.3             0.1   \n",
       "1           0.4             3.0             1.0             0.4   \n",
       "\n",
       "   search_S_alphas  search_S_gammas  search_S_thetas  tv_S_alphas  \\\n",
       "0              0.5              0.3              0.0          0.5   \n",
       "1              3.0              1.0              0.3          3.0   \n",
       "\n",
       "   tv_S_gammas  tv_S_thetas  train_size  lambda  \n",
       "0          0.3          0.3         0.5       0  \n",
       "1          1.0          0.8         0.8       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data from JSON\n",
    "loaded_data = load_data_from_json(\n",
    "    \"/Users/funny/Downloads/exported_data.json\"\n",
    ")\n",
    "imported_data = import_data(loaded_data)\n",
    "model_outputs = imported_data[\"model_outputs\"]\n",
    "display((model_outputs.hyper_bound_ng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyperparameters(hyperparameters={'facebook_S_alphas': [0.5, 3], 'facebook_S_gammas': [0.3, 1], 'facebook_S_thetas': [0, 0.3], 'print_S_alphas': [0.5, 3], 'print_S_gammas': [0.3, 1], 'print_S_thetas': [0.1, 0.4], 'tv_S_alphas': [0.5, 3], 'tv_S_gammas': [0.3, 1], 'tv_S_thetas': [0.3, 0.8], 'search_S_alphas': [0.5, 3], 'search_S_gammas': [0.3, 1], 'search_S_thetas': [0, 0.3], 'ooh_S_alphas': [0.5, 3], 'ooh_S_gammas': [0.3, 1], 'ooh_S_thetas': [0.1, 0.4], 'newsletter_alphas': [0.5, 3], 'newsletter_gammas': [0.3, 1], 'newsletter_thetas': [0.1, 0.4], 'train_size': [0.5, 0.8]}, adstock=<AdstockType.GEOMETRIC: 'geometric'>, lambda_=0.0, train_size=(0.5, 0.8), hyper_bound_list_updated={'facebook_S_alphas': [0.5, 3], 'facebook_S_gammas': [0.3, 1], 'facebook_S_thetas': [0, 0.3], 'print_S_alphas': [0.5, 3], 'print_S_gammas': [0.3, 1], 'print_S_thetas': [0.1, 0.4], 'tv_S_alphas': [0.5, 3], 'tv_S_gammas': [0.3, 1], 'tv_S_thetas': [0.3, 0.8], 'search_S_alphas': [0.5, 3], 'search_S_gammas': [0.3, 1], 'search_S_thetas': [0, 0.3], 'ooh_S_alphas': [0.5, 3], 'ooh_S_gammas': [0.3, 1], 'ooh_S_thetas': [0.1, 0.4], 'newsletter_alphas': [0.5, 3], 'newsletter_gammas': [0.3, 1], 'newsletter_thetas': [0.1, 0.4], 'train_size': [0.5, 0.8]})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "mmm_data = imported_data[\"mmm_data\"]\n",
    "# display(mmm_data.data.head())\n",
    "# Display Model Outputs\n",
    "\n",
    "model_outputs = imported_data[\"model_outputs\"]\n",
    "# display(len(model_outputs.trials))\n",
    "# display((model_outputs.trials[0].result_hyp_param))\n",
    "# display((model_outputs.trials[0].decomp_spend_dist))\n",
    "# display((model_outputs.trials[0].x_decomp_agg))\n",
    "# display((model_outputs.trials[0].lift_calibration))\n",
    "\n",
    "hyperparameters = imported_data[\"hyperparameters\"]\n",
    "display(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pareto_fronts 1\n",
      "pareto_fronts_vec [1] 50175 50175\n",
      ">>> Calculating response curves for all models' media variables (50175)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Create ParetoOptimizer instance\n",
    "pareto_optimizer = ParetoOptimizer(mmm_data, model_outputs, hyperparameters)\n",
    "\n",
    "# 4. Run optimize function\n",
    "pareto_result = pareto_optimizer.optimize(pareto_fronts=\"auto\", min_candidates=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Check results\n",
    "print(\"Pareto Optimization Results:\")\n",
    "print(f\"Number of Pareto solutions: {len(pareto_result.pareto_solutions)}\")\n",
    "print(f\"Number of Pareto fronts: {pareto_result.pareto_fronts}\")\n",
    "print(\"\\nPareto-optimal solutions:\")\n",
    "print(pareto_result.result_hyp_param[['solID', 'nrmse', 'decomp.rssd', 'mape', 'robynPareto']])\n",
    "\n",
    "print(\"\\nAggregated decomposition results:\")\n",
    "print(pareto_result.x_decomp_agg.head())\n",
    "\n",
    "print(\"\\nMedia vector collection:\")\n",
    "print(pareto_result.media_vec_collect.head())\n",
    "\n",
    "print(\"\\nDecomposition vector collection:\")\n",
    "print(pareto_result.x_decomp_vec_collect.head())\n",
    "\n",
    "# 6. Validate logic\n",
    "assert len(pareto_result.pareto_solutions) > 0, \"No Pareto-optimal solutions found\"\n",
    "assert pareto_result.pareto_fronts > 0, \"Invalid number of Pareto fronts\"\n",
    "assert not pareto_result.result_hyp_param.empty, \"Empty result_hyp_param DataFrame\"\n",
    "assert not pareto_result.x_decomp_agg.empty, \"Empty x_decomp_agg DataFrame\"\n",
    "assert not pareto_result.media_vec_collect.empty, \"Empty media_vec_collect DataFrame\"\n",
    "assert not pareto_result.x_decomp_vec_collect.empty, \"Empty x_decomp_vec_collect DataFrame\"\n",
    "\n",
    "print(\"\\nAll assertions passed. The optimize function is working as expected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
